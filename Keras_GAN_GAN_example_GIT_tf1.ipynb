{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_GAN_GAN_example_GIT_tf1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jvallalta/ia3/blob/main/Keras_GAN_GAN_example_GIT_tf1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj9oh6HGnGoa"
      },
      "source": [
        "# Keras-GAN\n",
        "https://github.com/eriklindernoren/Keras-GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmITZlpwn3HV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e199e3ce-d0e6-40f4-b24c-0f8c8eb012cb"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2hI9lfdpnmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc21795-47b3-4cda-a79d-e28e2a30e423"
      },
      "source": [
        "!git clone https://github.com/eriklindernoren/Keras-GAN\n",
        "%cd Keras-GAN/\n",
        "!sudo pip3 install -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Keras-GAN'...\n",
            "remote: Enumerating objects: 3483, done.\u001b[K\n",
            "remote: Total 3483 (delta 0), reused 0 (delta 0), pack-reused 3483\u001b[K\n",
            "Receiving objects: 100% (3483/3483), 88.40 MiB | 46.66 MiB/s, done.\n",
            "Resolving deltas: 100% (564/564), done.\n",
            "/content/Keras-GAN\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git (from -r requirements.txt (line 2))\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-gb0zpnh5\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-gb0zpnh5\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (2.4.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (7.0.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (0.16.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 9)) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2.5)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->-r requirements.txt (line 9)) (4.4.2)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101066 sha256=b6950584ac678a1274a03a4ede21b6f483b1d2ba18edd7ef24f23ab9909d29ea\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jblaidqq/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGKMsA2dpnqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9486c0e6-d5c5-4f88-9387-482a320d33a3"
      },
      "source": [
        "%cd 'gan'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Keras-GAN/gan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq09PeVpW6_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08ecf42-ab80-4ca5-e322-684990683016"
      },
      "source": [
        "from gan import GAN"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgFxB79Wq4kV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14860274-a08a-412e-9008-9ec9a237762e"
      },
      "source": [
        "model = GAN()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 784)               803600    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRcBAbkJq8nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1762bee6-da9e-4ac6-ab89-e496914e8ff4"
      },
      "source": [
        "model.train(epochs=200, batch_size=32, sample_interval=200)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.903480, acc.: 14.06%] [G loss: 0.753243]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 [D loss: 0.394510, acc.: 76.56%] [G loss: 0.790382]\n",
            "2 [D loss: 0.355708, acc.: 81.25%] [G loss: 0.812799]\n",
            "3 [D loss: 0.308542, acc.: 92.19%] [G loss: 0.866779]\n",
            "4 [D loss: 0.312267, acc.: 90.62%] [G loss: 0.939830]\n",
            "5 [D loss: 0.282879, acc.: 93.75%] [G loss: 1.112224]\n",
            "6 [D loss: 0.261961, acc.: 96.88%] [G loss: 1.137481]\n",
            "7 [D loss: 0.215276, acc.: 100.00%] [G loss: 1.376667]\n",
            "8 [D loss: 0.186944, acc.: 100.00%] [G loss: 1.513484]\n",
            "9 [D loss: 0.157882, acc.: 100.00%] [G loss: 1.678944]\n",
            "10 [D loss: 0.126271, acc.: 100.00%] [G loss: 1.717916]\n",
            "11 [D loss: 0.106952, acc.: 100.00%] [G loss: 1.964270]\n",
            "12 [D loss: 0.094822, acc.: 100.00%] [G loss: 2.048414]\n",
            "13 [D loss: 0.084498, acc.: 100.00%] [G loss: 2.159455]\n",
            "14 [D loss: 0.080910, acc.: 100.00%] [G loss: 2.313118]\n",
            "15 [D loss: 0.070451, acc.: 100.00%] [G loss: 2.316403]\n",
            "16 [D loss: 0.068545, acc.: 100.00%] [G loss: 2.479008]\n",
            "17 [D loss: 0.054979, acc.: 100.00%] [G loss: 2.702852]\n",
            "18 [D loss: 0.045843, acc.: 100.00%] [G loss: 2.702147]\n",
            "19 [D loss: 0.053444, acc.: 100.00%] [G loss: 2.813743]\n",
            "20 [D loss: 0.043089, acc.: 100.00%] [G loss: 2.901919]\n",
            "21 [D loss: 0.043048, acc.: 100.00%] [G loss: 2.857126]\n",
            "22 [D loss: 0.049418, acc.: 100.00%] [G loss: 2.963378]\n",
            "23 [D loss: 0.038734, acc.: 100.00%] [G loss: 3.063083]\n",
            "24 [D loss: 0.040405, acc.: 100.00%] [G loss: 3.118339]\n",
            "25 [D loss: 0.032572, acc.: 100.00%] [G loss: 3.153789]\n",
            "26 [D loss: 0.032567, acc.: 100.00%] [G loss: 3.115443]\n",
            "27 [D loss: 0.028618, acc.: 100.00%] [G loss: 3.214284]\n",
            "28 [D loss: 0.026928, acc.: 100.00%] [G loss: 3.245155]\n",
            "29 [D loss: 0.031566, acc.: 100.00%] [G loss: 3.363143]\n",
            "30 [D loss: 0.028537, acc.: 100.00%] [G loss: 3.374850]\n",
            "31 [D loss: 0.022110, acc.: 100.00%] [G loss: 3.417247]\n",
            "32 [D loss: 0.026254, acc.: 100.00%] [G loss: 3.434983]\n",
            "33 [D loss: 0.023470, acc.: 100.00%] [G loss: 3.449872]\n",
            "34 [D loss: 0.023474, acc.: 100.00%] [G loss: 3.486293]\n",
            "35 [D loss: 0.021527, acc.: 100.00%] [G loss: 3.588718]\n",
            "36 [D loss: 0.023220, acc.: 100.00%] [G loss: 3.672553]\n",
            "37 [D loss: 0.019925, acc.: 100.00%] [G loss: 3.593089]\n",
            "38 [D loss: 0.022062, acc.: 100.00%] [G loss: 3.665299]\n",
            "39 [D loss: 0.025096, acc.: 100.00%] [G loss: 3.631466]\n",
            "40 [D loss: 0.020509, acc.: 100.00%] [G loss: 3.688031]\n",
            "41 [D loss: 0.018326, acc.: 100.00%] [G loss: 3.825079]\n",
            "42 [D loss: 0.022180, acc.: 100.00%] [G loss: 3.940872]\n",
            "43 [D loss: 0.014247, acc.: 100.00%] [G loss: 3.923776]\n",
            "44 [D loss: 0.020858, acc.: 100.00%] [G loss: 3.939120]\n",
            "45 [D loss: 0.019300, acc.: 100.00%] [G loss: 3.978750]\n",
            "46 [D loss: 0.021028, acc.: 100.00%] [G loss: 4.057111]\n",
            "47 [D loss: 0.016151, acc.: 100.00%] [G loss: 4.116391]\n",
            "48 [D loss: 0.016062, acc.: 100.00%] [G loss: 4.116555]\n",
            "49 [D loss: 0.022250, acc.: 100.00%] [G loss: 4.105132]\n",
            "50 [D loss: 0.011739, acc.: 100.00%] [G loss: 3.998963]\n",
            "51 [D loss: 0.019466, acc.: 100.00%] [G loss: 4.128287]\n",
            "52 [D loss: 0.014155, acc.: 100.00%] [G loss: 4.185475]\n",
            "53 [D loss: 0.016035, acc.: 100.00%] [G loss: 4.293076]\n",
            "54 [D loss: 0.018957, acc.: 100.00%] [G loss: 4.166108]\n",
            "55 [D loss: 0.019609, acc.: 100.00%] [G loss: 4.155204]\n",
            "56 [D loss: 0.017725, acc.: 100.00%] [G loss: 4.454303]\n",
            "57 [D loss: 0.015296, acc.: 100.00%] [G loss: 4.405972]\n",
            "58 [D loss: 0.015522, acc.: 100.00%] [G loss: 4.395665]\n",
            "59 [D loss: 0.011894, acc.: 100.00%] [G loss: 4.400007]\n",
            "60 [D loss: 0.014390, acc.: 100.00%] [G loss: 4.391171]\n",
            "61 [D loss: 0.011596, acc.: 100.00%] [G loss: 4.299837]\n",
            "62 [D loss: 0.009246, acc.: 100.00%] [G loss: 4.253393]\n",
            "63 [D loss: 0.015344, acc.: 100.00%] [G loss: 4.460660]\n",
            "64 [D loss: 0.008104, acc.: 100.00%] [G loss: 4.312656]\n",
            "65 [D loss: 0.013066, acc.: 100.00%] [G loss: 4.327600]\n",
            "66 [D loss: 0.010055, acc.: 100.00%] [G loss: 4.230521]\n",
            "67 [D loss: 0.019962, acc.: 100.00%] [G loss: 4.256365]\n",
            "68 [D loss: 0.013589, acc.: 100.00%] [G loss: 4.494275]\n",
            "69 [D loss: 0.009818, acc.: 100.00%] [G loss: 4.348906]\n",
            "70 [D loss: 0.008433, acc.: 100.00%] [G loss: 4.375754]\n",
            "71 [D loss: 0.010682, acc.: 100.00%] [G loss: 4.294813]\n",
            "72 [D loss: 0.011457, acc.: 100.00%] [G loss: 4.328575]\n",
            "73 [D loss: 0.013523, acc.: 100.00%] [G loss: 4.540597]\n",
            "74 [D loss: 0.013663, acc.: 100.00%] [G loss: 4.343410]\n",
            "75 [D loss: 0.012943, acc.: 100.00%] [G loss: 4.542667]\n",
            "76 [D loss: 0.016552, acc.: 100.00%] [G loss: 4.493568]\n",
            "77 [D loss: 0.009666, acc.: 100.00%] [G loss: 4.453736]\n",
            "78 [D loss: 0.011062, acc.: 100.00%] [G loss: 4.451554]\n",
            "79 [D loss: 0.010693, acc.: 100.00%] [G loss: 4.355976]\n",
            "80 [D loss: 0.015530, acc.: 100.00%] [G loss: 4.321566]\n",
            "81 [D loss: 0.013001, acc.: 100.00%] [G loss: 4.367401]\n",
            "82 [D loss: 0.013070, acc.: 100.00%] [G loss: 4.484936]\n",
            "83 [D loss: 0.016651, acc.: 100.00%] [G loss: 4.731858]\n",
            "84 [D loss: 0.019684, acc.: 100.00%] [G loss: 4.700549]\n",
            "85 [D loss: 0.011946, acc.: 100.00%] [G loss: 4.726674]\n",
            "86 [D loss: 0.009330, acc.: 100.00%] [G loss: 4.494334]\n",
            "87 [D loss: 0.015458, acc.: 100.00%] [G loss: 4.676146]\n",
            "88 [D loss: 0.012836, acc.: 100.00%] [G loss: 4.394285]\n",
            "89 [D loss: 0.015468, acc.: 100.00%] [G loss: 4.656692]\n",
            "90 [D loss: 0.013960, acc.: 100.00%] [G loss: 4.716762]\n",
            "91 [D loss: 0.010507, acc.: 100.00%] [G loss: 4.722271]\n",
            "92 [D loss: 0.014051, acc.: 100.00%] [G loss: 4.801206]\n",
            "93 [D loss: 0.012893, acc.: 100.00%] [G loss: 4.847543]\n",
            "94 [D loss: 0.008426, acc.: 100.00%] [G loss: 4.821287]\n",
            "95 [D loss: 0.008883, acc.: 100.00%] [G loss: 4.539890]\n",
            "96 [D loss: 0.016958, acc.: 100.00%] [G loss: 4.820508]\n",
            "97 [D loss: 0.008431, acc.: 100.00%] [G loss: 4.848221]\n",
            "98 [D loss: 0.015092, acc.: 100.00%] [G loss: 4.623941]\n",
            "99 [D loss: 0.016566, acc.: 100.00%] [G loss: 4.911336]\n",
            "100 [D loss: 0.023144, acc.: 100.00%] [G loss: 4.926983]\n",
            "101 [D loss: 0.008694, acc.: 100.00%] [G loss: 4.765882]\n",
            "102 [D loss: 0.009655, acc.: 100.00%] [G loss: 4.662953]\n",
            "103 [D loss: 0.010503, acc.: 100.00%] [G loss: 4.821248]\n",
            "104 [D loss: 0.010512, acc.: 100.00%] [G loss: 4.585536]\n",
            "105 [D loss: 0.026512, acc.: 100.00%] [G loss: 4.774048]\n",
            "106 [D loss: 0.028975, acc.: 100.00%] [G loss: 5.085417]\n",
            "107 [D loss: 0.019665, acc.: 100.00%] [G loss: 5.148076]\n",
            "108 [D loss: 0.019479, acc.: 100.00%] [G loss: 4.942863]\n",
            "109 [D loss: 0.011243, acc.: 100.00%] [G loss: 4.981725]\n",
            "110 [D loss: 0.035495, acc.: 98.44%] [G loss: 5.677352]\n",
            "111 [D loss: 0.371183, acc.: 85.94%] [G loss: 4.257013]\n",
            "112 [D loss: 0.032476, acc.: 100.00%] [G loss: 5.148170]\n",
            "113 [D loss: 0.007856, acc.: 100.00%] [G loss: 5.604077]\n",
            "114 [D loss: 0.019356, acc.: 100.00%] [G loss: 4.989393]\n",
            "115 [D loss: 0.031931, acc.: 100.00%] [G loss: 4.936905]\n",
            "116 [D loss: 0.030614, acc.: 100.00%] [G loss: 4.928320]\n",
            "117 [D loss: 0.026450, acc.: 100.00%] [G loss: 5.602647]\n",
            "118 [D loss: 0.065594, acc.: 100.00%] [G loss: 4.790897]\n",
            "119 [D loss: 0.048250, acc.: 98.44%] [G loss: 5.351024]\n",
            "120 [D loss: 0.074882, acc.: 98.44%] [G loss: 5.315299]\n",
            "121 [D loss: 0.111959, acc.: 93.75%] [G loss: 5.139005]\n",
            "122 [D loss: 0.039018, acc.: 98.44%] [G loss: 5.197273]\n",
            "123 [D loss: 0.035821, acc.: 100.00%] [G loss: 5.061281]\n",
            "124 [D loss: 0.103220, acc.: 96.88%] [G loss: 5.308628]\n",
            "125 [D loss: 0.395187, acc.: 85.94%] [G loss: 4.422577]\n",
            "126 [D loss: 0.037211, acc.: 98.44%] [G loss: 5.469537]\n",
            "127 [D loss: 0.413537, acc.: 79.69%] [G loss: 3.920805]\n",
            "128 [D loss: 0.124777, acc.: 93.75%] [G loss: 4.712134]\n",
            "129 [D loss: 0.042951, acc.: 100.00%] [G loss: 4.997638]\n",
            "130 [D loss: 0.172161, acc.: 92.19%] [G loss: 4.166107]\n",
            "131 [D loss: 0.120094, acc.: 95.31%] [G loss: 5.075303]\n",
            "132 [D loss: 0.066384, acc.: 98.44%] [G loss: 5.523639]\n",
            "133 [D loss: 0.458615, acc.: 84.38%] [G loss: 3.517745]\n",
            "134 [D loss: 0.093930, acc.: 95.31%] [G loss: 4.267628]\n",
            "135 [D loss: 0.053859, acc.: 100.00%] [G loss: 4.920584]\n",
            "136 [D loss: 0.214803, acc.: 93.75%] [G loss: 3.367342]\n",
            "137 [D loss: 0.121986, acc.: 93.75%] [G loss: 3.709304]\n",
            "138 [D loss: 0.062255, acc.: 98.44%] [G loss: 4.485334]\n",
            "139 [D loss: 0.119127, acc.: 95.31%] [G loss: 4.305093]\n",
            "140 [D loss: 0.154871, acc.: 93.75%] [G loss: 4.509518]\n",
            "141 [D loss: 0.103663, acc.: 95.31%] [G loss: 4.863255]\n",
            "142 [D loss: 0.170180, acc.: 93.75%] [G loss: 4.093900]\n",
            "143 [D loss: 0.093058, acc.: 95.31%] [G loss: 4.557836]\n",
            "144 [D loss: 0.292039, acc.: 89.06%] [G loss: 3.146475]\n",
            "145 [D loss: 0.233527, acc.: 87.50%] [G loss: 4.882505]\n",
            "146 [D loss: 0.401318, acc.: 79.69%] [G loss: 3.701525]\n",
            "147 [D loss: 0.055484, acc.: 98.44%] [G loss: 4.618524]\n",
            "148 [D loss: 0.346248, acc.: 89.06%] [G loss: 3.893652]\n",
            "149 [D loss: 0.120573, acc.: 93.75%] [G loss: 4.442788]\n",
            "150 [D loss: 0.331593, acc.: 85.94%] [G loss: 3.888015]\n",
            "151 [D loss: 0.057744, acc.: 100.00%] [G loss: 4.256354]\n",
            "152 [D loss: 0.669723, acc.: 79.69%] [G loss: 3.144210]\n",
            "153 [D loss: 0.066991, acc.: 98.44%] [G loss: 3.309556]\n",
            "154 [D loss: 0.072387, acc.: 98.44%] [G loss: 3.462015]\n",
            "155 [D loss: 0.075509, acc.: 98.44%] [G loss: 3.367961]\n",
            "156 [D loss: 0.160663, acc.: 92.19%] [G loss: 4.177758]\n",
            "157 [D loss: 0.630698, acc.: 73.44%] [G loss: 2.722685]\n",
            "158 [D loss: 0.114526, acc.: 96.88%] [G loss: 3.256037]\n",
            "159 [D loss: 0.079501, acc.: 98.44%] [G loss: 3.849897]\n",
            "160 [D loss: 0.092049, acc.: 98.44%] [G loss: 3.819541]\n",
            "161 [D loss: 0.080659, acc.: 96.88%] [G loss: 3.918919]\n",
            "162 [D loss: 0.266760, acc.: 82.81%] [G loss: 4.531211]\n",
            "163 [D loss: 0.180433, acc.: 95.31%] [G loss: 3.217695]\n",
            "164 [D loss: 0.129743, acc.: 95.31%] [G loss: 4.096471]\n",
            "165 [D loss: 0.146713, acc.: 93.75%] [G loss: 4.581300]\n",
            "166 [D loss: 0.609889, acc.: 68.75%] [G loss: 2.852106]\n",
            "167 [D loss: 0.105425, acc.: 95.31%] [G loss: 4.176232]\n",
            "168 [D loss: 0.136275, acc.: 96.88%] [G loss: 3.412944]\n",
            "169 [D loss: 0.077558, acc.: 98.44%] [G loss: 3.294407]\n",
            "170 [D loss: 0.143763, acc.: 95.31%] [G loss: 4.062427]\n",
            "171 [D loss: 0.485066, acc.: 78.12%] [G loss: 3.094558]\n",
            "172 [D loss: 0.077748, acc.: 98.44%] [G loss: 4.354451]\n",
            "173 [D loss: 0.478831, acc.: 78.12%] [G loss: 2.421314]\n",
            "174 [D loss: 0.151936, acc.: 90.62%] [G loss: 3.061635]\n",
            "175 [D loss: 0.074322, acc.: 100.00%] [G loss: 4.503052]\n",
            "176 [D loss: 0.245107, acc.: 87.50%] [G loss: 2.950847]\n",
            "177 [D loss: 0.150681, acc.: 92.19%] [G loss: 3.537075]\n",
            "178 [D loss: 0.125875, acc.: 98.44%] [G loss: 4.546222]\n",
            "179 [D loss: 0.237109, acc.: 90.62%] [G loss: 3.931811]\n",
            "180 [D loss: 0.104324, acc.: 98.44%] [G loss: 3.606054]\n",
            "181 [D loss: 0.202119, acc.: 93.75%] [G loss: 3.596382]\n",
            "182 [D loss: 0.360027, acc.: 85.94%] [G loss: 3.803494]\n",
            "183 [D loss: 0.421767, acc.: 78.12%] [G loss: 3.057662]\n",
            "184 [D loss: 0.133906, acc.: 93.75%] [G loss: 3.751825]\n",
            "185 [D loss: 0.386047, acc.: 81.25%] [G loss: 3.185781]\n",
            "186 [D loss: 0.122940, acc.: 95.31%] [G loss: 3.971036]\n",
            "187 [D loss: 0.434828, acc.: 81.25%] [G loss: 2.935075]\n",
            "188 [D loss: 0.146405, acc.: 93.75%] [G loss: 4.127712]\n",
            "189 [D loss: 1.055070, acc.: 57.81%] [G loss: 2.110008]\n",
            "190 [D loss: 0.230288, acc.: 87.50%] [G loss: 3.863346]\n",
            "191 [D loss: 0.110270, acc.: 100.00%] [G loss: 4.218203]\n",
            "192 [D loss: 0.254776, acc.: 92.19%] [G loss: 3.139691]\n",
            "193 [D loss: 0.092949, acc.: 98.44%] [G loss: 3.640262]\n",
            "194 [D loss: 0.107204, acc.: 95.31%] [G loss: 3.631666]\n",
            "195 [D loss: 0.908965, acc.: 59.38%] [G loss: 2.763077]\n",
            "196 [D loss: 0.112704, acc.: 96.88%] [G loss: 4.001006]\n",
            "197 [D loss: 0.618166, acc.: 65.62%] [G loss: 2.221062]\n",
            "198 [D loss: 0.220460, acc.: 87.50%] [G loss: 3.953133]\n",
            "199 [D loss: 0.239985, acc.: 93.75%] [G loss: 3.346133]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po309LHJnlFx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
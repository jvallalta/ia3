{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-Pytorch-Regresion_Logistica.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ghPSdPkCrLyk",
        "n2QOf-HEltUP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "df7c51f5d065449e9b9df7a2fbdc870f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_60c765d8246142dabadfbff3100fde08",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_29418aaa384743b7921dd0e889dfda10",
              "IPY_MODEL_45352f24787e4f6b9a7aa1d901e300c3"
            ]
          }
        },
        "60c765d8246142dabadfbff3100fde08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29418aaa384743b7921dd0e889dfda10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_69c6f1fc5c3f4894b12c6fa8d4d47208",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7fd578a60b47418d928f58cda1505f33"
          }
        },
        "45352f24787e4f6b9a7aa1d901e300c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1d9469d9dcde4acbbab139c95ce4b001",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:20&lt;00:00, 13341458.04it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e3d199e60fca40d3a3b69e46fbf83014"
          }
        },
        "69c6f1fc5c3f4894b12c6fa8d4d47208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7fd578a60b47418d928f58cda1505f33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1d9469d9dcde4acbbab139c95ce4b001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e3d199e60fca40d3a3b69e46fbf83014": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "244f8e6eb097458d8cf5ce990c4641f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d01821fb1f80471594424a651567e8e3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_64e02e01ba1341ddbc62f37e3c7e7777",
              "IPY_MODEL_25b837df7be2463b883f9c50a4bf8297"
            ]
          }
        },
        "d01821fb1f80471594424a651567e8e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "64e02e01ba1341ddbc62f37e3c7e7777": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8383eaacb6694e578f9cab3ffbfed3f4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60c4489bd3e8493d892008b7e3a8ddf5"
          }
        },
        "25b837df7be2463b883f9c50a4bf8297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5260ae9eee12412f86a04cf347b59470",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:00&lt;00:00, 72077.57it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_632f8acf1a604747bd6ff0f911bffbd3"
          }
        },
        "8383eaacb6694e578f9cab3ffbfed3f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60c4489bd3e8493d892008b7e3a8ddf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5260ae9eee12412f86a04cf347b59470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "632f8acf1a604747bd6ff0f911bffbd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc2cabe1833e4febb47237fb0ed67b33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ae6f1f3b55dd4090920ae4564e28bee7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2c604dce52ff4e5d856872844eb40e6b",
              "IPY_MODEL_6152b6ebb78042c985b26fc9bc538b94"
            ]
          }
        },
        "ae6f1f3b55dd4090920ae4564e28bee7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c604dce52ff4e5d856872844eb40e6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8d7e959092834006942ed189e1664733",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a92e1b75afdc445c996916e83c0c6c1b"
          }
        },
        "6152b6ebb78042c985b26fc9bc538b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6198a1dc752b4546b7dac880d69af012",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:17&lt;00:00, 11796266.25it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8220445019a343c786191148def2e50c"
          }
        },
        "8d7e959092834006942ed189e1664733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a92e1b75afdc445c996916e83c0c6c1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6198a1dc752b4546b7dac880d69af012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8220445019a343c786191148def2e50c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec78c07fabec4a5188ffce4a6891edd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_74413b839a4a4dc1af094e3ac68ff177",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3887150312c54e4aa1911ecc983a22a9",
              "IPY_MODEL_2d313040fc9f461989bfce6c68f6ce99"
            ]
          }
        },
        "74413b839a4a4dc1af094e3ac68ff177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3887150312c54e4aa1911ecc983a22a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_99d34417e003401b9e2beab99681b478",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04f29c90736740b08a049258f0190406"
          }
        },
        "2d313040fc9f461989bfce6c68f6ce99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d9362208b0b44b64916065352d5572a1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/? [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_711213fbd53745d29cfae689f707744f"
          }
        },
        "99d34417e003401b9e2beab99681b478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04f29c90736740b08a049258f0190406": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9362208b0b44b64916065352d5572a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "711213fbd53745d29cfae689f707744f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jvallalta/ia3/blob/main/03_Pytorch_Regresion_Logistica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghPSdPkCrLyk"
      },
      "source": [
        "![IDAL](https://i.imgur.com/tIKXIG1.jpg)  \n",
        "\n",
        "#<strong>**MÃ¡ster en Inteligencia Artificial Avanzada y Aplicada  IA^3**</strong>\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxh4k85SpESm"
      },
      "source": [
        "# <center>**RegresiÃ³n logÃ­stica con Pytorch**</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL4niY96fTTV"
      },
      "source": [
        "En este notebook vamos a estudiar los siguientes aspectos: \n",
        "\n",
        "* Trabajar con imagenes en Pytorch (empleando el dataset MNIST)\n",
        "* Dividir el conjunto de datos en entrenamiento, test y validaciÃ³n.\n",
        "* Crear modelos en Pytorch a partir de la clase base `nn.Module` y personalizandola. \n",
        "* Interpretar las salidas del modelo coomo probabilidades empleando la fnciÃ³n softmax y asignar etiquetas en predicciÃ³n\n",
        "* Emplear mÃ©tricas de evaluaciÃ³n y funciones de error adecuadas para problemas de clasificaciÃ³n\n",
        "* Desarrollar el proceso de entrernamiento que tambiÃ©n evalue el modelo con cel conjunto de validaciÃ³n\n",
        "* Probar el modelo manualmente con ejemplos \n",
        "* Salvar y cargar el modelo para evitar reentrenar desde cero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlizBg8UfTTV"
      },
      "source": [
        "## Trabajando con imagenes\n",
        "\n",
        "En este ejemplo vamos a aprovechar lo que ya sabemos sobre Pytorch y la regresion lineal para aplicarlo a un tipo de problema diferente: *clasificaciÃ³n* en este caso de imÃ¡genes. Para ello vamos a usar la conocida [*MNIST Handwritten Digits Database*](http://yann.lecun.com/exdb/mnist/) como nuestro conjunto de datos. Consiste en imÃ¡genes de 28px por 28px en escala de grises de dÃ­gitos escritos a mano (0 a 9) y sus respectivas etiquetas indicando que dÃ­gito representa cada imagen. \n",
        "\n",
        "El dataset tiene este aspecto:\n",
        "\n",
        "![mnist-sample](https://i.imgur.com/CAYnuo1.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IArll2smfTTV"
      },
      "source": [
        "Empezaremos por importar `torch` y `torchvision`. `torchvision` contiene utilidades para trabajar con imÃ¡genes y clases muy Ãºtiles para descargar e importar automÃ¡ticamente conjuntos de datos habituales como MNIST.\r\n",
        "\r\n",
        "(Puedes consultar la [documentaciÃ³n](https://pytorch.org/docs/stable/torchvision/datasets.html) para ver todos los datasets disponibles a traves de esta librerÃ­a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_D8CGr0fTTV"
      },
      "source": [
        "# Si ejecutas el notebook de forma local, descomenta la linea que corresponda a tu sistema operativo para instalar las libreriÃ¡s correspondientes\n",
        "\n",
        "# Linux / Binder\n",
        "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Windows\n",
        "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# MacOS\n",
        "# !pip install numpy matplotlib torch torchvision torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_xViEWvfTTV"
      },
      "source": [
        "# Importamos\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386,
          "referenced_widgets": [
            "df7c51f5d065449e9b9df7a2fbdc870f",
            "60c765d8246142dabadfbff3100fde08",
            "29418aaa384743b7921dd0e889dfda10",
            "45352f24787e4f6b9a7aa1d901e300c3",
            "69c6f1fc5c3f4894b12c6fa8d4d47208",
            "7fd578a60b47418d928f58cda1505f33",
            "1d9469d9dcde4acbbab139c95ce4b001",
            "e3d199e60fca40d3a3b69e46fbf83014",
            "244f8e6eb097458d8cf5ce990c4641f8",
            "d01821fb1f80471594424a651567e8e3",
            "64e02e01ba1341ddbc62f37e3c7e7777",
            "25b837df7be2463b883f9c50a4bf8297",
            "8383eaacb6694e578f9cab3ffbfed3f4",
            "60c4489bd3e8493d892008b7e3a8ddf5",
            "5260ae9eee12412f86a04cf347b59470",
            "632f8acf1a604747bd6ff0f911bffbd3",
            "bc2cabe1833e4febb47237fb0ed67b33",
            "ae6f1f3b55dd4090920ae4564e28bee7",
            "2c604dce52ff4e5d856872844eb40e6b",
            "6152b6ebb78042c985b26fc9bc538b94",
            "8d7e959092834006942ed189e1664733",
            "a92e1b75afdc445c996916e83c0c6c1b",
            "6198a1dc752b4546b7dac880d69af012",
            "8220445019a343c786191148def2e50c",
            "ec78c07fabec4a5188ffce4a6891edd6",
            "74413b839a4a4dc1af094e3ac68ff177",
            "3887150312c54e4aa1911ecc983a22a9",
            "2d313040fc9f461989bfce6c68f6ce99",
            "99d34417e003401b9e2beab99681b478",
            "04f29c90736740b08a049258f0190406",
            "d9362208b0b44b64916065352d5572a1",
            "711213fbd53745d29cfae689f707744f"
          ]
        },
        "id": "q1Skz7dlfTTV",
        "outputId": "2f7dce23-954c-4cf6-a089-b3592c28dd77"
      },
      "source": [
        "# Descargamos el training dataset\n",
        "dataset = MNIST(root='data/', download=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df7c51f5d065449e9b9df7a2fbdc870f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "244f8e6eb097458d8cf5ce990c4641f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc2cabe1833e4febb47237fb0ed67b33",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec78c07fabec4a5188ffce4a6891edd6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Koi-fVbAfTTV"
      },
      "source": [
        "Una vez ejecutado el cÃ³digo anterior, los datos han sido descargados al directorio `data/` vinculado al notebook  y se ha creado un dataset PyTorch `Dataset`. En siguientes ejecuciones, la descarga serÃ¡ omitida ya que los datos ya estÃ¡n descargados. \r\n",
        "\r\n",
        "Comprobamos el tamaÃ±o del dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9mWyKqAfTTV",
        "outputId": "62dcd437-1e22-4b21-fa5c-575fae0c23bf"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pRKB16NfTTW"
      },
      "source": [
        "El dataset tiene 60,000 imagenes que vamos a emplear para entrenar el modelo. Disponemos tambiÃ©n de un conjunto adicional de test de 10,000 imagenes que serÃ¡ usado para evaluar el modelo y establecer mÃ©tricas en artÃ­culos y/o informes. \r\n",
        "\r\n",
        "Podemos crear el conjunto de test empleando la clase `MNIST` pasando el argumento `train=False` al constructor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLqZQOlWfTTW",
        "outputId": "2948c829-7205-45c0-9be4-af9531927ec9"
      },
      "source": [
        "test_dataset = MNIST(root='data/', train=False)\n",
        "len(test_dataset)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgJru-f7fTTW"
      },
      "source": [
        "Observemos un ejemplo del training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZmSZeFGfTTW",
        "outputId": "917b0178-aa59-40dd-cd1c-0b8200eabc5a"
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=L size=28x28 at 0x7F5BAE249588>, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oBdGAc4fTTX"
      },
      "source": [
        "Vemos que el ejemplo consiste en un par de elementos. Una imagen de 28x28 y una etiqueta indicando el dÃ­gito que representa. La imagen es un objeto de la clase `PIL.Image.Image`, la cual es parte de la *Python imaging library* [Pillow](https://pillow.readthedocs.io/en/stable/). Podemos visualizar la imagen en Jupyter usando [`matplotlib`](https://matplotlib.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edyBIgXPfTTX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSVQL5vJfTTX"
      },
      "source": [
        "La instrucciÃ³n `%matplotlib inline` indica a Jupyter que queremos que muestre el grÃ¡fico en el propio notebook. Sin esa lÃ­nea, la imagen se mostrarÃ­a en una ventana popup. Las instrucciones que se inician con `%` son las denominadas *magic commands* y se emplean para configurar el comportamiento del propio Jupyter. Y Puedes encontrar el listado completo de estos comandos aquÃ­: https://ipython.readthedocs.io/en/stable/interactive/magics.html .\n",
        "\n",
        "Veamos un par de ejemplos del dataset: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "3cxwpTQLfTTX",
        "outputId": "13524ef7-47bc-4fc1-fac6-e9d08b189fe8"
      },
      "source": [
        "image, label = dataset[2]\n",
        "plt.imshow(image, cmap='gray')\n",
        "print('Label:', label)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "35ELPbM7fTTX",
        "outputId": "eab0eda8-2889-4ac2-f215-c35f7881ed09"
      },
      "source": [
        "image, label = dataset[1499]\n",
        "plt.imshow(image, cmap='gray')\n",
        "print('Label:', label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vO2Gh9RfTTX"
      },
      "source": [
        "Observamos que las imagenes que tenemos son relativamente pequeÃ±as y por tanto el reconocimento del dÃ­gito puede llegar a ser dificil, incluso para un ojo humano. No obstante, este dataset no deja de ser muy Ãºtil. \r\n",
        "\r\n",
        "Llegado este puntom el principal problema es que Pytorcho no sabe como trabajar con imagenes. De forma similar a otras librerÃ­as DL, serÃ¡ necesario transformar las imÃ¡genes a tensores. Podemos realizar esto directamente cuando creamos el dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBnad_UDfTTX"
      },
      "source": [
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "084T0emtfTTX"
      },
      "source": [
        "PyTorch datasets nos permite especificar una o mÃ¡s transformaciones que se aplican a las imagenes directamente cuando son cargadas. El mÃ³dulo `torchvision.transforms` contiene esas funciones predefinidas. Vamos a emplear la transformaciÃ³n `ToTensor` para convertir las imÃ¡genes a tensores PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYvBQ3LzfTTX"
      },
      "source": [
        "# MNIST dataset (imagenes and labels)\n",
        "dataset = MNIST(root='data/', \n",
        "                train=True,\n",
        "                transform=transforms.ToTensor())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCZ4h7jMfTTX",
        "outputId": "21febdd9-a614-4a28-c35d-006c1cffe885"
      },
      "source": [
        "img_tensor, label = dataset[0]\n",
        "print(img_tensor.shape, label)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28]) 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io317DfSfTTX"
      },
      "source": [
        "Ahora la imagen ya es un tensor de dimensiones 1x28x28. La primera dimensiÃ³n indica los canales de color. La segunda y la tercera implican la altura y anchura de la imagen, respectivamente. Como las imÃ±agenes son en escala de grises, solo hay un canal de color. Otros conjuntos de datos con imÃ¡genes en colo tendrÃ­an 3 canales: rojo, verde y azul (RGB).\n",
        "\n",
        "Veamos algunos valores del interior del tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prZFgdtefTTX",
        "outputId": "225d72f0-acc5-4ab2-f4cd-0bcbe92fc2b2"
      },
      "source": [
        "print(img_tensor[0,10:15,10:15])\n",
        "print(torch.max(img_tensor), torch.min(img_tensor))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
            "        [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
            "        [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
            "        [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
            "        [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]])\n",
            "tensor(1.) tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2HjQ5P5fTTX"
      },
      "source": [
        "La segunda lÃ­nea de cÃ³digo nos muestra el rango de valores, que va de 0 a 1. O representa el negro, 1 el blanco,  y los valores intermedios representan los diferentes tonos de grises. \r\n",
        "\r\n",
        "Veamos grÃ¡ficamente el tensor como imagen empleando `plt.imshow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Fw-NvsqpfTTX",
        "outputId": "3de8d710-536d-494b-b29b-b98b4d030ca2"
      },
      "source": [
        "plt.imshow(img_tensor[0,10:15,10:15], cmap='gray');"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAJMElEQVR4nO3d34uUhR7H8c/nrEZRB7qwi3BFIyKQ4BSIBF4EQmQWdVtg3VR7cwKDIOqyfyC66WapSEiMoC6iOoSQEUFWW22SWWA/DhmB5yBa3RTmp4sZDh7ZdZ8Z55lnni/vFyzs7AwzH2TfPjOzy7NOIgB1/K3rAQAmi6iBYogaKIaogWKIGihmXRt3ars3b6lv3ry56wkj2bBhQ9cTRvL99993PaGxU6dOdT1hJEm80tfdxo+0bMde8fFmzuLiYtcTRvLwww93PWEke/bs6XpCY/v37+96wkhWi5qn30AxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDGNora9y/Y3to/bfrLtUQDGt2bUtuckPSfpTklbJd1ve2vbwwCMp8mReruk40m+S/KHpFck3dvuLADjahL1Rkk/nnf5xPBr/8f2gu0l20uTGgdgdBM7RXCSRUmLUr9OEQxU0+RI/ZOkTeddnh9+DcAMahL1J5JusH2d7csk3SfpjXZnARjXmk+/k5y1/aikdyTNSXoxydHWlwEYS6PX1EnelvR2y1sATAC/UQYUQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDETO/HghZJ+nHvwzJkzXU8o7ZFHHul6QmMHDhzoekJj586dW/U6jtRAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxa0Zt+0XbJ21/OY1BAC5NkyP1S5J2tbwDwISsGXWS9yWdmsIWABPAa2qgmImdTdT2gqSFSd0fgPFMLOoki5IWJcl2P84PDBTE02+gmCY/0jog6UNJN9o+Yfuh9mcBGNeaT7+T3D+NIQAmg6ffQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0U42TypxPr0znKrrzyyq4njOStt97qesJIbrvttq4nNHbHHXd0PaGxw4cP68yZM17pOo7UQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFLNm1LY32T5k+yvbR23vncYwAONZ1+A2ZyU9nuQz23+X9Kntg0m+ankbgDGseaRO8nOSz4af/yrpmKSNbQ8DMJ4mR+r/sb1F0i2SPlrhugVJCxNZBWBsjaO2fZWk1yQ9luSXC69PsihpcXjb3pwiGKim0bvfttdrEPT+JK+3OwnApWjy7rclvSDpWJJn2p8E4FI0OVLvkPSApJ22l4cfu1veBWBMa76mTvKBpBX/vAeA2cNvlAHFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UIyTyZ8jkBMPtuf666/vesJIlpeXu57Q2OnTp7ue0Nju3bt15MiRFU9ewpEaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBooZs2obV9u+2PbX9g+avvpaQwDMJ51DW7zu6SdSX6zvV7SB7b/leRwy9sAjGHNqDM4idlvw4vrhx+cgwyYUY1eU9ues70s6aSkg0k+ancWgHE1ijrJn0luljQvabvtmy68je0F20u2lyY9EkBzI737neS0pEOSdq1w3WKSbUm2TWocgNE1eff7GttXDz+/QtLtkr5uexiA8TR59/taSftsz2nwn8CrSd5sdxaAcTV59/uIpFumsAXABPAbZUAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFNPkzCeYId9++23XE0by4IMPdj2hsX379nU9obF161ZPlyM1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxTSO2vac7c9tv9nmIACXZpQj9V5Jx9oaAmAyGkVte17SXZKeb3cOgEvV9Ej9rKQnJJ1b7Qa2F2wv2V6ayDIAY1kzatt3SzqZ5NOL3S7JYpJtSbZNbB2AkTU5Uu+QdI/tHyS9Immn7ZdbXQVgbGtGneSpJPNJtki6T9K7Sfa0vgzAWPg5NVDMSH92J8l7kt5rZQmAieBIDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMU4y+Tu1/yPp3xO+2w2S/jvh+2xTn/b2aavUr71tbd2c5JqVrmgl6jbYXurTmUr7tLdPW6V+7e1iK0+/gWKIGiimT1Evdj1gRH3a26etUr/2Tn1rb15TA2imT0dqAA0QNVBML6K2vcv2N7aP236y6z0XY/tF2ydtf9n1lrXY3mT7kO2vbB+1vbfrTauxfbntj21/Mdz6dNebmrA9Z/tz229O6zFnPmrbc5Kek3SnpK2S7re9tdtVF/WSpF1dj2jorKTHk2yVdKukf87wv+3vknYm+YekmyXtsn1rx5ua2Cvp2DQfcOajlrRd0vEk3yX5Q4O/vHlvx5tWleR9Sae63tFEkp+TfDb8/FcNvvk2drtqZRn4bXhx/fBjpt/ltT0v6S5Jz0/zcfsQ9UZJP553+YRm9Buvz2xvkXSLpI+6XbK64VPZZUknJR1MMrNbh56V9ISkc9N80D5EjZbZvkrSa5IeS/JL13tWk+TPJDdLmpe03fZNXW9aje27JZ1M8um0H7sPUf8kadN5l+eHX8ME2F6vQdD7k7ze9Z4mkpyWdEiz/d7FDkn32P5Bg5eMO22/PI0H7kPUn0i6wfZ1ti/T4A/fv9HxphJsW9ILko4leabrPRdj+xrbVw8/v0LS7ZK+7nbV6pI8lWQ+yRYNvmffTbJnGo8981EnOSvpUUnvaPBGzqtJjna7anW2D0j6UNKNtk/YfqjrTRexQ9IDGhxFlocfu7setYprJR2yfUSD/+gPJpnaj4n6hF8TBYqZ+SM1gNEQNVAMUQPFEDVQDFEDxRA1UAxRA8X8BY427AI3W9MfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSKTZ8aifTTX"
      },
      "source": [
        "## Conjuntos de entrenamiento, test y validaciÃ³n\n",
        "\n",
        "Como ya sabemos, cuando construimos modelos reales de aprendizaje mÃ¡quina, es habitual dividir el conjunto de datos en tres partes: \n",
        "\n",
        "1. **Training set** - empleado para entrenar el modelo. Es con el que calculamos el error y usamos para ajustar los pesos empleando el algoritmo de optimizaciÃ³n elegido (p.e. descenso de gradiente).\n",
        "2. **Validation set** - se emplea para evaluar el modelo durante el entrenamiento, ajustar los hiperparÃ¡metros (como learning rate, etc.) y de esa forma escoger la mejor versiÃ³n obtenida. \n",
        "3. **Test set** - empleado para comparar diferentes modelos entrenados y para aproximar el modelo a su eficacia final con nuevos datos. .\n",
        "\n",
        "En el conjunto MNIST tenemos 60.000 imÃ¡genes para entrenamiento y 10.000 para test. El conjunto de test estÃ¡ especÃ­ficamente preparado, de forma que diferentes investigadores puedan comprobar y reportar sus resultados contra la misma colecciÃ³n de imagenes.  \n",
        "\n",
        "Dado que no hay un conjunto predefinido de validaciÃ³n, lo vamos a obtener nosotros manualmente dividiendo el conjunto de entrenamiento. Vamos a separar 10.000 imÃ¡genes aleatoriamente para validaciÃ³n. Para ello emplearemos el mÃ©todo `random_spilt` de PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dcE0OPBfTTX",
        "outputId": "fb5772bc-f504-45a3-a67f-252dd3b1d8d7"
      },
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
        "len(train_ds), len(val_ds)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CACcvMnZfTTX"
      },
      "source": [
        "Es fundamental que la divisiÃ³n de los conjuntos sea **aleatoria**. A menudo, el conjunto de entrenamiento puede estar ordenado de alguna forma. Si nos limitaramos a tomar el ultimo 20% de las imÃ¡genes, podriamos estar dejando fuera tanto de entrenamiento como de validaciÃ³n algunas etiquetas. Esto nos llevarÃ­a a un modelo defectuoso, que estarÃ­a mal entrenado y no serÃ­a nada Ãºtil.\n",
        "\n",
        "Llegado este punto ya podemos crear data loaders que nos van ayudar a cargar los datos en lotes. En este caso emplearemos lotes de 128 elementos. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_oD7BTfTTX"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCtbVhkOfTTX"
      },
      "source": [
        "Recordemos que hemos de poner `shuffle=True` para el dataLoader de entrenamiento de forma que aseguremos que los lotes que se generan en cada ciclo (epoch) son diferentes. Esta alatorizaciÃ³n ayuda a entrenar mÃ¡s rÃ¡pido y a generalizar mejor. Por otro lado, dado que el conjunto de validaciÃ³n Ãºnicamente es emmpleado para evaluar el modelo, no es necesario barajar las imÃ¡genes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy-lajKQfTTY"
      },
      "source": [
        "## Modelo\n",
        "\n",
        "A continuaciÃ³n vamos a preparar nuestro modelo, teniendo en cuenta las siguientes consideraciones: \n",
        "\n",
        "* Un modelo de  **regresiÃ³n logÃ­stica** es practicamente idÃ©ntico a un modlo de regresiÃ³n lineal. Contiene matrices de pesos y biases y su salida se obtiene realizando las mismas operaciones bÃ¡sicas (`pred = x @ w.t() + b`). \n",
        "\n",
        "* De igual forma que con la regresion lineal, podemos emplear la clase base `nn.Linear` para crear el modelo en lugar de crear e inicializar manualmente las matrices.\n",
        "\n",
        "* Dado que `nn.Linear` espera que cada muestra de entrenamiento sea un vector, ahora cada tensor imagen de `1x28x28` tiene que ser \"extendido\" (_flattened_) a un vector de tamaÃ±o 784 `(28*28)` antes de ser pasado al modelo. \n",
        "\n",
        "* Finalmente, en este caso la salida para la imagen ya no es un solo valor, sino un vector de tamaÃ±o 10, en el que cada elemento significa la probabilidad de que el elemento de entrada sea esa etiqueta en particular (i.e., 0 al 9). La etiqueta predicha serÃ¡ simplemente aquella con el valor de probabilidad mÃ¡s alto. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzVbD7pifTTY"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input_size = 28*28\n",
        "num_classes = 10\n",
        "\n",
        "# Modelo de regresiÃ³n logÃ­stica\n",
        "model = nn.Linear(input_size, num_classes)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ilamccMfTTY"
      },
      "source": [
        "Este modelo es mucho mÃ¡s grande que el modelo anterior en tÃ©rminos del nÃºmero de parametros que hay que ajustar (ahora tenemos 10 salidas en lugar de una). \r\n",
        "\r\n",
        "Veamos los pesos y biases: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAj8gVdUfTTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9570b2ae-09d7-4e1a-b5ea-13006467740e"
      },
      "source": [
        "print(model.weight.shape)\n",
        "model.weight"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 784])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0335, -0.0251, -0.0098,  ...,  0.0028,  0.0233, -0.0193],\n",
              "        [-0.0096, -0.0291, -0.0228,  ...,  0.0172, -0.0143,  0.0071],\n",
              "        [-0.0322, -0.0187, -0.0300,  ...,  0.0322, -0.0003,  0.0090],\n",
              "        ...,\n",
              "        [ 0.0168,  0.0041,  0.0072,  ...,  0.0007, -0.0032, -0.0339],\n",
              "        [-0.0127, -0.0162,  0.0133,  ..., -0.0122,  0.0340, -0.0210],\n",
              "        [-0.0322, -0.0289, -0.0016,  ...,  0.0348,  0.0020,  0.0081]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRnviQ34fTTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9795043a-38c6-4889-c47b-e6bb15c17ffc"
      },
      "source": [
        "print(model.bias.shape)\n",
        "model.bias"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.0261, -0.0307, -0.0295, -0.0255,  0.0080, -0.0215,  0.0088, -0.0171,\n",
              "        -0.0161,  0.0253], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is6yNefafTTY"
      },
      "source": [
        "Aunque ahora tenemos un total de 7850 parametros, conceptualmente no hay nada muy diferente. Vamos a generar algunos resultados con el modelo que hemos definido. \r\n",
        "Vamos a emplear el primer lote de 128 imÃ¡genes del dataset y lo pasamos a nuestro modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIuxYOy3fTTY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "d9bc3f43-897b-4c58-d1be-e07c4bcba5f0"
      },
      "source": [
        "for images, labels in train_loader:\n",
        "    print(labels)\n",
        "    print(images.shape)\n",
        "    outputs = model(images)\n",
        "    print(outputs)\n",
        "    break"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([7, 0, 1, 1, 1, 6, 1, 5, 0, 8, 1, 7, 5, 3, 2, 7, 8, 7, 8, 9, 1, 8, 6, 2,\n",
            "        1, 1, 9, 7, 8, 3, 4, 2, 0, 3, 6, 3, 0, 1, 7, 9, 8, 1, 3, 2, 3, 8, 5, 7,\n",
            "        0, 9, 4, 7, 7, 1, 0, 6, 0, 9, 5, 7, 1, 8, 1, 9, 8, 0, 1, 6, 7, 2, 1, 6,\n",
            "        1, 6, 4, 3, 6, 5, 5, 8, 4, 7, 9, 0, 8, 8, 3, 3, 3, 8, 5, 2, 2, 7, 3, 7,\n",
            "        1, 1, 7, 1, 7, 2, 4, 3, 2, 4, 1, 9, 2, 1, 5, 5, 5, 6, 4, 3, 2, 7, 6, 9,\n",
            "        9, 7, 8, 7, 9, 5, 6, 4])\n",
            "torch.Size([128, 1, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d0fe7d306f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m-JsBr6lSO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43a1231-f07e-4efa-9f06-aa460fbcbb26"
      },
      "source": [
        "images.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3_ktSR1lVJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45e8d439-9d24-4b1a-88e9-2977c0572956"
      },
      "source": [
        "images.reshape(128, 784).shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 784])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lT2ZhMjfTTY"
      },
      "source": [
        "El cÃ³dio anterior nos ha producido erro porque los datos de entrada no estÃ¡n en la forma correcta. Nuestras imÃ¡genes son 1x28x28, pero nosotros las necesitamos como vectores de tamaÃ±o 784, i.e. necesitamos \"extenderlas\". Para ello emplearemos el mÃ©todo de tensores `.reshape`, que nos va a permitir eficientemente generar una vista de cada imagen como un vector, sin realmente crear una copia de los datos.  \n",
        "\n",
        "Para incluir esta funcionalidad en nuestro modelo, necesitamos definir un modelo personalizado extendiendo la clase `nn.Module` de PyTorch. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2HWol7qfTTY"
      },
      "source": [
        "class MnistModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        xb = xb.reshape(-1, 784)\n",
        "        out = self.linear(xb)\n",
        "        return out\n",
        "    \n",
        "model = MnistModel()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZKn3GBFfTTY"
      },
      "source": [
        "En el interior del constructor `__init__`, lo que hacemos ens instanciar los pesos y biases empleando `nn.Linear`, de igual forma que lo que hemos visto anteriormente. A continuaciÃ³n preparamos un nuevo mÃ©todo `forward`, el cual toma un lote de entradas cuando es invocado, lo \"extiende\" al formato de tensor de entrada correcto, y lo pasa a la funciÃ³n `self.linear`.\n",
        "\n",
        "`xb.reshape(-1, 28*28)` indica a PyTorch que quremos una *vista* del tensor `xb` en dos dimensiones. La longitud de la segunda dimensiÃ³n la hemos especificado nosotros (28x28=784). Un argumento de `.reshape` puede ser puesto a `-1` (en nuestro caso, la primera dimensiÃ³n) para hacer que PyTorch calcule automÃ¡ticamente el valor necesario, basado en la forma especificada.\n",
        "\n",
        "NÃ³tese que el modelo ya no tiene los atributos `.weight` y `.bias` (ahora estÃ¡n dentro del attributo `.linear`), pero sÃ­ que tiene un mÃ©todo `.parameters` que retorna na lista conteniendo los pesos y biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voZF2M_3nVnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d195548-de41-49d0-9797-68cc19da138e"
      },
      "source": [
        "model.linear"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=784, out_features=10, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUlyKm3pPMmS",
        "outputId": "65ef7b9a-3cb5-4370-c74c-9ad0c1f9cc36"
      },
      "source": [
        "model.linear.bias\r\n",
        "\r\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([ 0.0124, -0.0287,  0.0037, -0.0268, -0.0101,  0.0248, -0.0048, -0.0208,\n",
              "         0.0282,  0.0097], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JciejPo9fTTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0957e2b1-9ea5-499c-a717-03e5eceee5e8"
      },
      "source": [
        "print(model.linear.weight.shape, model.linear.bias.shape)\n",
        "list(model.parameters())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 784]) torch.Size([10])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.0037, -0.0310, -0.0225,  ..., -0.0188,  0.0167,  0.0254],\n",
              "         [ 0.0128, -0.0073,  0.0003,  ...,  0.0234,  0.0283, -0.0092],\n",
              "         [ 0.0193, -0.0011,  0.0292,  ..., -0.0308, -0.0280,  0.0133],\n",
              "         ...,\n",
              "         [ 0.0188, -0.0047,  0.0059,  ..., -0.0146,  0.0322, -0.0246],\n",
              "         [ 0.0220,  0.0325, -0.0002,  ..., -0.0228,  0.0142, -0.0203],\n",
              "         [-0.0356, -0.0122,  0.0173,  ..., -0.0036,  0.0233,  0.0008]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 0.0124, -0.0287,  0.0037, -0.0268, -0.0101,  0.0248, -0.0048, -0.0208,\n",
              "          0.0282,  0.0097], requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGvgCQn2fTTY"
      },
      "source": [
        "Ahora podemos usar el modelo que hemos definido. Veamos si funciona correctamente: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ6FGOvYfTTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06543af-57db-40bd-bb17-fb59831e2e28"
      },
      "source": [
        "for images, labels in train_loader:\n",
        "    print(images.shape)\n",
        "    outputs = model(images)\n",
        "    break\n",
        "\n",
        "print('outputs.shape : ', outputs.shape)\n",
        "print('Sample outputs :\\n', outputs[:2].data)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 28, 28])\n",
            "outputs.shape :  torch.Size([128, 10])\n",
            "Sample outputs :\n",
            " tensor([[ 5.2753e-01, -2.1475e-01, -1.0958e-01,  1.4950e-04,  1.7668e-01,\n",
            "         -6.3830e-01, -1.7347e-01,  1.6723e-01, -2.6118e-01,  2.9498e-02],\n",
            "        [-2.0242e-02, -1.4736e-01, -1.3137e-01,  1.3729e-02,  1.6410e-01,\n",
            "         -2.1138e-01,  4.1514e-02,  1.6834e-03, -2.1475e-01,  2.1246e-01]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v407S1jfTTZ"
      },
      "source": [
        "Para cada una de las 128 imagenes, ahora tenemos 10 salidas, una por cada clase. Como se ha indicado anteriormente, tomaremos esos valores de salida como posibilidades. Para ello, los valores de salida deben estar entre el rango 0 y 1, lo cual no es el caso. \n",
        "\n",
        "Para realizar la transformaciÃ³n de los valores obtenidos a probabilidades, emplearemos la funciÃ³n sofmax, que tiene la siguiente fÃ³rmula: \n",
        "\n",
        "![softmax](https://i.imgur.com/EAh9jLN.png)\n",
        "\n",
        "Primero reemplazamos cada elemento `yi` de una fila de salida en `e^yi`, haciendo todos los elementos positivos. \n",
        "\n",
        "![](https://www.montereyinstitute.org/courses/DevelopmentalMath/COURSE_TEXT2_RESOURCE/U18_L1_T1_text_final_6_files/image001.png)\n",
        "\n",
        "A continuaciÃ³n dividimos cada uno entre la suma de todos, para asegurar que no superamos el valor 1. El vector vestor resultante ya puede ser interpretado como probabilidades.\n",
        "\n",
        "A pesar de que implementar directamente la funciÃ³n sofmax no es dificil, Pytorch ya cuenta con una implementaciÃ³n propia que ademÃ¡s funciona muy bien con tensores multidimensionales (p.e. una lista de filas de salida en nuestro caso)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc7d6WD0fTTZ"
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbYF4auWfTTZ"
      },
      "source": [
        "La funciÃ³n softmax estÃ¡ incluidoa en el paquete `torch.nn.functional` y requiere que le especifiquemos la dimensiÃ³n a lo largo de la cual debe ser aplicada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xvjcBVTo14n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "404604de-47b7-4679-c9cc-d7bd37a1af08"
      },
      "source": [
        "outputs[:2]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.2753e-01, -2.1475e-01, -1.0958e-01,  1.4950e-04,  1.7668e-01,\n",
              "         -6.3830e-01, -1.7347e-01,  1.6723e-01, -2.6118e-01,  2.9498e-02],\n",
              "        [-2.0242e-02, -1.4736e-01, -1.3137e-01,  1.3729e-02,  1.6410e-01,\n",
              "         -2.1138e-01,  4.1514e-02,  1.6834e-03, -2.1475e-01,  2.1246e-01]],\n",
              "       grad_fn=<SliceBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3neZ1YGfTTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61b99129-dcdc-41d7-c061-c24f6e52f214"
      },
      "source": [
        "# Aplicamos softmax para cada fila de salida\n",
        "probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "# Miramos las probabilidades de unas muestras\n",
        "print(\"Sample probabilities:\\n\", probs[:2].data)\n",
        "\n",
        "# Comprobamos la suma de probabilidades de una fila:\n",
        "print(\"Sum: \", torch.sum(probs[0]).item())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample probabilities:\n",
            " tensor([[0.1705, 0.0811, 0.0901, 0.1006, 0.1200, 0.0531, 0.0846, 0.1189, 0.0775,\n",
            "         0.1036],\n",
            "        [0.0999, 0.0880, 0.0894, 0.1034, 0.1201, 0.0825, 0.1063, 0.1021, 0.0822,\n",
            "         0.1261]])\n",
            "Sum:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2cgqWGVfTTZ"
      },
      "source": [
        "Finalmente elegiremos la etiqueta de salida simplemente seleccionando el Ã­ndice de la que mayores probabilidades ha obtenido en cada fila (vector) de salida. Para ello podemos usar `torch.max`, que retorna para cada fila, el mayor elemento y su correspondiente Ã­ndice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o9a4hAufTTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ac4fbf-8fc3-44c0-91b8-5b5cca197f99"
      },
      "source": [
        "max_probs, preds = torch.max(probs, dim=1)\n",
        "print(preds)\n",
        "print(max_probs)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 9, 9, 4, 0, 4, 2, 0, 4, 4, 1, 0, 3, 8, 4, 0, 0, 8, 0, 0, 7, 7, 0, 0,\n",
            "        4, 0, 7, 9, 0, 4, 4, 5, 0, 0, 7, 7, 4, 4, 0, 8, 0, 2, 7, 6, 0, 0, 4, 7,\n",
            "        7, 0, 7, 0, 0, 3, 2, 7, 4, 4, 7, 7, 0, 4, 4, 8, 7, 0, 9, 6, 0, 0, 4, 6,\n",
            "        9, 0, 9, 8, 4, 3, 4, 0, 4, 0, 8, 0, 4, 4, 7, 7, 4, 9, 7, 7, 4, 1, 7, 8,\n",
            "        1, 4, 7, 0, 0, 3, 0, 7, 8, 8, 3, 7, 7, 0, 4, 7, 4, 2, 0, 6, 3, 9, 7, 7,\n",
            "        8, 0, 7, 4, 2, 4, 9, 0])\n",
            "tensor([0.1705, 0.1261, 0.1110, 0.1312, 0.1374, 0.1124, 0.1364, 0.1405, 0.1355,\n",
            "        0.1256, 0.1358, 0.1063, 0.1418, 0.1210, 0.1480, 0.1203, 0.1309, 0.1150,\n",
            "        0.1228, 0.1539, 0.1401, 0.1422, 0.1382, 0.1518, 0.1147, 0.1507, 0.1202,\n",
            "        0.1208, 0.1271, 0.1247, 0.1237, 0.1211, 0.1426, 0.1537, 0.1379, 0.1196,\n",
            "        0.1376, 0.1269, 0.1187, 0.1263, 0.1237, 0.1117, 0.1216, 0.1135, 0.1353,\n",
            "        0.1247, 0.1187, 0.1348, 0.1223, 0.1334, 0.1148, 0.1264, 0.1264, 0.1510,\n",
            "        0.1336, 0.1199, 0.1249, 0.1303, 0.1143, 0.1248, 0.1361, 0.1471, 0.1290,\n",
            "        0.1224, 0.1293, 0.1303, 0.1457, 0.1174, 0.1322, 0.1612, 0.1525, 0.1163,\n",
            "        0.1161, 0.1292, 0.1400, 0.1178, 0.1203, 0.1331, 0.1410, 0.1161, 0.1544,\n",
            "        0.1366, 0.1263, 0.1169, 0.1214, 0.1164, 0.1202, 0.1205, 0.1402, 0.1202,\n",
            "        0.1259, 0.1375, 0.1162, 0.1275, 0.1239, 0.1159, 0.1449, 0.1363, 0.1323,\n",
            "        0.1376, 0.1188, 0.1285, 0.1349, 0.1447, 0.1272, 0.1129, 0.1233, 0.1505,\n",
            "        0.1106, 0.1330, 0.1225, 0.1186, 0.1262, 0.1135, 0.1189, 0.1245, 0.1248,\n",
            "        0.1245, 0.1298, 0.1268, 0.1230, 0.1382, 0.1622, 0.1367, 0.1292, 0.1166,\n",
            "        0.1218, 0.1259], grad_fn=<MaxBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGvKME9WfTTZ"
      },
      "source": [
        "Los nÃºmeros mostrados arriba son las etiquetas predichas para el primer lote de entrenamiento. Vamos a compararlas con las reales. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsT85mhffTTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc32a00-6a03-4f14-99ad-033973571e6c"
      },
      "source": [
        "labels"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 4, 7, 5, 3, 1, 9, 3, 5, 0, 0, 7, 7, 1, 5, 0, 2, 1, 4, 6, 9, 0, 8, 3,\n",
              "        1, 6, 6, 9, 2, 1, 1, 4, 6, 6, 0, 9, 1, 8, 8, 1, 3, 5, 0, 7, 9, 1, 1, 0,\n",
              "        0, 8, 5, 0, 7, 2, 6, 7, 0, 3, 4, 5, 3, 3, 3, 7, 5, 3, 4, 7, 8, 0, 3, 1,\n",
              "        9, 2, 9, 2, 1, 8, 1, 9, 5, 2, 1, 2, 8, 6, 5, 9, 8, 9, 3, 6, 1, 0, 3, 8,\n",
              "        0, 7, 5, 2, 9, 2, 2, 8, 8, 3, 8, 5, 4, 9, 1, 2, 8, 7, 9, 4, 2, 4, 9, 6,\n",
              "        1, 8, 6, 5, 9, 7, 9, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AFZgI8SfTTZ"
      },
      "source": [
        "La mayorÃ­a de las etiquetas de predicciÃ³n son diferentes de las reales. Esto es porque hemos empezado con unos valores de pesos y biases elegidos al azar. Evidentemente, necesitamos entrenar el modelo para ajustar los pesos y realizar mejores predicciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FegT9vhlfTTZ"
      },
      "source": [
        "## MÃ©trica de evaluaciÃ³n y funciÃ³n de error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSiZ3KtqfTTZ"
      },
      "source": [
        "De igual forma que en la regresiÃ³n linealm necesitamos una forma para evaluar como de bien estÃ¡ haciendolo nuestro modelo. Una forma bÃ¡sica y muy natural consiste en obtener el porcentaje de etiquetas que son correctamente predichas, i.e. la **precision** (**accuracy**)  de las predicciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG0IBPVMp3UE",
        "outputId": "df618e16-370d-40eb-b54e-b9fc50375732"
      },
      "source": [
        "outputs[:2]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.2753e-01, -2.1475e-01, -1.0958e-01,  1.4950e-04,  1.7668e-01,\n",
              "         -6.3830e-01, -1.7347e-01,  1.6723e-01, -2.6118e-01,  2.9498e-02],\n",
              "        [-2.0242e-02, -1.4736e-01, -1.3137e-01,  1.3729e-02,  1.6410e-01,\n",
              "         -2.1138e-01,  4.1514e-02,  1.6834e-03, -2.1475e-01,  2.1246e-01]],\n",
              "       grad_fn=<SliceBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHgMnCL4qGDt",
        "outputId": "5f422642-7545-47a2-baf9-102940f15023"
      },
      "source": [
        "torch.sum(preds == labels)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11hDOTR1fTTZ"
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E258dVOUfTTZ"
      },
      "source": [
        "El  operador `==` realiza una comparaciÃ³n elemento por elemento de los dos tensores que tienen la misma forma y retorna un tensor de resultados de la misma forma, conteniendo `True` para elementos iguales y `False` para los diferentes. Pasando el resultado a `torch.sum` nos retorna el nÃºmero total de etiquetas que se han predicho correctamente. Finalmente dividimos entre el numero total de etiquetas evaluadas para obtener la precisiÃ³n. \n",
        "\n",
        "NÃ³tese que en este caso no es necesario aplicar la funciÃ³n softmax al vector de resultados. Esto es asÃ­ porque `e^x` es una funciÃ³n incremental, i.e., si `y1 > y2`, entonces `e^y1 > e^y2`. Lo mismo se cumple despues de promediar los valores para obtener el softmax. Por tanto podemos saltarnos esa transformaciÃ³n tomando directamente el valor mÃ¡ximo del vector de salida.\n",
        "\n",
        "Calculamos la precision del modelo para el primer lote de datos (sin entrenar)\n",
        "Let's calculate the accuracy of the current model on the first batch of data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwVAxpZXfTTZ",
        "outputId": "42307d96-cf66-4417-89b0-6ee243c0e051"
      },
      "source": [
        "accuracy(outputs, labels)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0859)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEyaxGIBrNFm",
        "outputId": "ffd8fbe7-60f9-4fd9-c1ed-18913f575bc4"
      },
      "source": [
        "probs"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1705, 0.0811, 0.0901,  ..., 0.1189, 0.0775, 0.1036],\n",
              "        [0.0999, 0.0880, 0.0894,  ..., 0.1021, 0.0822, 0.1261],\n",
              "        [0.1020, 0.0935, 0.1088,  ..., 0.0965, 0.0804, 0.1110],\n",
              "        ...,\n",
              "        [0.1156, 0.0825, 0.0944,  ..., 0.0927, 0.0894, 0.1060],\n",
              "        [0.1103, 0.0899, 0.1043,  ..., 0.1179, 0.0837, 0.1218],\n",
              "        [0.1259, 0.1005, 0.0886,  ..., 0.0950, 0.0870, 0.1094]],\n",
              "       grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te7ORtqVfTTZ"
      },
      "source": [
        "La precisiÃ³n es una buena forma de evaluar el modelo para nosotros (como humanos). Sin embargo, no la podemos emplear para optimizar el modelo empleando la tÃ©cnica de descenso de gradiente por las siguientes razones: \n",
        "\n",
        "1. No es una funciÃ³n diferenciable. Tanto `torch.max` como `==` son operaciones no-continuas y no-diferenciables. Por tanto no podemos calcular gradientes con repecto a los pesos y los biases, que nos puedan servir para actualizar los valores en la direcciÃ³n correcta.\n",
        "\n",
        "2. No tiene en cuenta las probabilidades obtenidas por el modelo, lo que hace que no podamos proveer de suficiente realimentaciÃ³n al modelo para mejoras incrementales. \n",
        "\n",
        "Por esas razones, la precision es utilizada habitualemente como **mÃ©trica de evaluacion** pero no como funciÃ³n de pÃ©rdida (tambien llamada funciÃ³n de coste). La funciÃ³n de pÃ©rdida comÃºnmente empleada en problemas de clasificaciÃ³n es la funciÃ³n de **entropÃ­a cruzada**  (_cross-entropy_), que tiene la siguiente fÃ³rmula: \n",
        "\n",
        "![cross-entropy](https://i.imgur.com/VDRDl1D.png)\n",
        "\n",
        "Aunque pueda parecer un poco complicada, en realidad es bastante simple:\n",
        "\n",
        "* Para cada vector de salida, tomamos el valor de probabilidad obtenido para la etiqueta correcta. E.g., si las probailidades obtenidas para una imagen son `[0.1, 0.3, 0.2, ...]` y la etiqueta correcta es `1`, tomaremos el valor `0.3` e inoramos el resto\n",
        "\n",
        "* A continuaciÃ³n obtenemos el [logaritmo](https://en.wikipedia.org/wiki/Logarithm) del valor de probailidad obtenida.  Si la probabilidad es alta, i.e. cercana a 1, entonces el valor obtenido serÃ¡ cercano a 0. Si la probabilidad es baja (cercana a 0) entonces el valor obtenido serÃ¡ un valor negativo alto. Dicho valor lo multiplicaremos por -1 para convertirlo en positivo. Esto resulta en que cuando la predicciÃ³n es baja obtenemos un valor alto que serÃ¡ tomado como la pÃ©rdida o error. Cuando la predicciÃ³n es alta, el error estarÃ¡ cercano a 0. \n",
        "\n",
        "![](https://www.intmath.com/blog/wp-content/images/2019/05/log10.png)\n",
        "\n",
        "* Finalmente para aprovechar la informaciÃ³n de todas las muestras empleadas, tomaremos el promedio de entropÃ­a cruzada de todas las filas de salida y tenemos una medida de la pÃ©rdida para todo un lote de datos. \n",
        "\n",
        "A diferencia de la precisiÃ³n. la entropÃ­a cruzada es continua y diferenciable. TambiÃ©n provee una informaciÃ³n de feedback muy Ãºtil para esas mejoras incrementales necesarias (mayores probabilidades se traducen en menor pÃ©rdida o error). Esas caracterÃ­sticas hacen que esta funciÃ³n sea una buena elecciÃ³n como funciÃ³n de coste para entrenar el modelo. \n",
        "\n",
        "Como era de esperar Pytorch tiene una implementaciÃ³n propia eficiente y preparada para tensores como parte del paquete `torch.nn.functional`. MÃ¡s aÃºn, tiene implementado internamente la funciÃ³n softmax, por lo que podemos pasarle directamente la salida del modelo sin tener que convertirlo en probavilidades. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkkyxKR1r8nm",
        "outputId": "ec81eb1b-1d85-4a16-b9ab-6985bfb80c16"
      },
      "source": [
        "outputs"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5275, -0.2148, -0.1096,  ...,  0.1672, -0.2612,  0.0295],\n",
              "        [-0.0202, -0.1474, -0.1314,  ...,  0.0017, -0.2148,  0.2125],\n",
              "        [ 0.0879,  0.0014,  0.1528,  ...,  0.0329, -0.1500,  0.1726],\n",
              "        ...,\n",
              "        [ 0.1756, -0.1621, -0.0275,  ..., -0.0452, -0.0816,  0.0883],\n",
              "        [ 0.1846, -0.0201,  0.1285,  ...,  0.2517, -0.0906,  0.2838],\n",
              "        [ 0.2162, -0.0090, -0.1344,  ..., -0.0655, -0.1528,  0.0757]],\n",
              "       grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqWUQpANfTTZ"
      },
      "source": [
        "loss_fn = F.cross_entropy"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuESG0hbfTTZ",
        "outputId": "be8e5dfb-2cd7-411b-8031-88f4becdc30d"
      },
      "source": [
        "# PÃ©rdida para el lote actual de datos\n",
        "loss = loss_fn(outputs, labels)\n",
        "print(loss)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3332, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFOtpxUqfTTZ"
      },
      "source": [
        "Ahora sabemos que la entropÃ­a cruzada es el negativo del logaritmo de la probabilidad predicha para la etiqueta correcta, promediado sobre todas las muestras. Por tanto, una forma en la que podemos interpretar el nÃºmero resultante obtenido e.g. `2.23` es que `e^-2.23` que es en torno a `0.1` como la probabilidad promedio de acierto de etiqueta correcta. En definitica *cuanto menor pÃ©rdida o error, mejor es el modelo*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTgl76MgfTTZ"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "\n",
        "Ahora que ya hemos definido la carga de datos, funciÃ³n de coste y optimizador, estamos listos para entrenar el modelo. El proceso es idÃ©ntico al de la regresiÃ³n lineal, con el aÃ±adido de que ahora tenemos una fase de validaciÃ³n para evaluar el modeloo en cada ciclo/epoch. \n",
        "\n",
        "Veamos como queda esto en pseudocÃ³digo:\n",
        "\n",
        "```\n",
        "for epoch in range(num_epochs):\n",
        "    # Fase de entrenamiento\n",
        "    for batch in train_loader:\n",
        "        # Genera predicciones\n",
        "        # Calcula el error/pÃ©rdida\n",
        "        # Calcula los gradientes c.r.a. los pesos y biases\n",
        "        # Actualiza los pesos\n",
        "        # Reseteamos los gradientes a cero \n",
        "    # Fase de validaciÃ³n\n",
        "    for batch in val_loader:\n",
        "        # Genera predictiones\n",
        "        # Calcula perdida\n",
        "        # Calcula metricas (accuracy etc.)\n",
        "    # Calcula promediado de loss & metricas\n",
        "    \n",
        "    # Log epoch, loss & metrics para inspecion\n",
        "```\n",
        "\n",
        "Algunas partes del bucle de entrenamiento son especÃ­ficas del problema que estamos enfrentando (e.g. loss function, metricas etc.) mientras otras son genÃ©ricas y pueden ser aplicadas a cualquier problema de aprendizaje mÃ¡quina. \n",
        "\n",
        "Lo que vamos a hacer es incluir las partes independientes del problema en una funciÃ³n que llamaremos `fit` y que emplearemos para entrenar el modelo. Las partes mÃ¡s especÃ­ficas las implementaremos aÃ±aiendo mÃ©todos a la clase base  `nn.Module` que definiremos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cv7a7ukfTTZ"
      },
      "source": [
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    history = [] # AquÃ­ guardaremos resultados por epochs\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        # Entrnamiento \n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        # ValidaciÃ³n\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "\n",
        "    return history"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAuhlro7fTTZ"
      },
      "source": [
        "En esta funciÃ³n `fit`  vamos a grabar la pÃ©rdida de validaciÃ³n y la mÃ©trica para cada epoch. Nos devuelve un histÃ³rico del entrenamiento realizado, lo cual serÃ¡ Ãºtil para visualizar y deputar. \n",
        "\n",
        "Configuraciones como el tamaÃ±o de lote, tasa de aprendizaje, etc. (llamados hiperparÃ¡metros) son crÃ­ticos para la obtenciÃ³n de buenos modelos y tiempos de entrenamiento razonables. Son un Ã¡rea activa de investigaciÃ³n por sÃ­ solos. Resulta interesante (y normalmente necesario) ir probando con ellospara ver como afecta al proceso y resultados de entrenmiento.\n",
        "\n",
        "Definimos ahora una funciÃ³n `evaluate` que emplearemos en la fase de validaciÃ³n de la function `fit`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N064K5H9ujtQ"
      },
      "source": [
        "l1 = [1, 2, 3, 4, 5]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W8a_hmFujqa",
        "outputId": "7e32c5d5-0531-403b-b31a-500b83ca524d"
      },
      "source": [
        "l2 = [x*2 for x in l1]\n",
        "l2"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 6, 8, 10]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgWjXs5mfTTZ"
      },
      "source": [
        "def evaluate(model, val_loader):\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdcKIXvMfTTZ"
      },
      "source": [
        "Finalmente vamos a redefinir la clase `MnistModel` para incluir una serie de mÃ©todos adicionales `training_step`, `validation_step`, `validation_epoch_end`, y `epoch_end` que aprovecharemos en las funciones `fit` y `evaluate`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbJN7ICGfTTZ"
      },
      "source": [
        "class MnistModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        xb = xb.reshape(-1, 784)\n",
        "        out = self.linear(xb)\n",
        "        return out\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Genera predicciones\n",
        "        loss = F.cross_entropy(out, labels) # Calcula loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Genera predicciones\n",
        "        loss = F.cross_entropy(out, labels)   # Calcula loss\n",
        "        acc = accuracy(out, labels)           # Calcula accuracy\n",
        "        return {'val_loss': loss, 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combina losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combina accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
        "    \n",
        "model = MnistModel()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-omw1RBfTTZ"
      },
      "source": [
        "Antes de entrenar nuestro nuevo modelo, vamos a ver que resultados obtiene con los pesos y biases inicializados aleatoriamente en la fase inicial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNUBy9qTfTTZ",
        "outputId": "b7b3fc3f-d668-4ae2-d3c8-750f5dfc46b1"
      },
      "source": [
        "result0 = evaluate(model, val_loader)\n",
        "result0"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.11837420612573624, 'val_loss': 2.2922189235687256}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KapQW0CDfTTZ"
      },
      "source": [
        "La precisiÃ³n inicial es en torno al 10%, que es lo que uno puede esperar de un modelo aleatorio teniendo en cuenta que tenemos 10 valores posibles de salida ( es decir, una posibilidad entre 10 de acertar al escoger aleatoriamente).\n",
        "\n",
        "Vamos a entrenar ahora el modelo. Configuramos 5 epochs inicialmente y observamos los resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQRahsa6fTTZ",
        "outputId": "78d5312b-9922-4397-89d7-9ce0f7d38bc7"
      },
      "source": [
        "history1 = fit(5, 0.001, model, train_loader, val_loader)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], val_loss: 1.9317, val_acc: 0.6368\n",
            "Epoch [1], val_loss: 1.6676, val_acc: 0.7388\n",
            "Epoch [2], val_loss: 1.4696, val_acc: 0.7703\n",
            "Epoch [3], val_loss: 1.3198, val_acc: 0.7872\n",
            "Epoch [4], val_loss: 1.2044, val_acc: 0.7993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mcIbHuKfTTa"
      },
      "source": [
        "Ahora tenemos un gran resultado! Simplemente hemos entrenado 5 ciclos y ya estamos en torno al 80% en el validation set. Veamos si epodemos mejorar mÃ¡s entrenando algunos ciclos mÃ¡s. Prueba a cambiar el nÃºmero de ciclos o learning rate en las celdas siguientes y observa los resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJvMd9CufTTa",
        "outputId": "d0659524-b22a-4a35-87ad-5dedec5942bf"
      },
      "source": [
        "history2 = fit(10, 0.001, model, train_loader, val_loader)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], val_loss: 1.1139, val_acc: 0.8071\n",
            "Epoch [1], val_loss: 1.0412, val_acc: 0.8137\n",
            "Epoch [2], val_loss: 0.9817, val_acc: 0.8196\n",
            "Epoch [3], val_loss: 0.9322, val_acc: 0.8236\n",
            "Epoch [4], val_loss: 0.8904, val_acc: 0.8276\n",
            "Epoch [5], val_loss: 0.8547, val_acc: 0.8316\n",
            "Epoch [6], val_loss: 0.8237, val_acc: 0.8340\n",
            "Epoch [7], val_loss: 0.7966, val_acc: 0.8372\n",
            "Epoch [8], val_loss: 0.7727, val_acc: 0.8396\n",
            "Epoch [9], val_loss: 0.7514, val_acc: 0.8416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hON9V8GBfTTa",
        "outputId": "74b684fe-790d-41f3-f136-7b5cf90a6e0b"
      },
      "source": [
        "history3 = fit(10, 0.001, model, train_loader, val_loader)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], val_loss: 0.7323, val_acc: 0.8432\n",
            "Epoch [1], val_loss: 0.7151, val_acc: 0.8451\n",
            "Epoch [2], val_loss: 0.6995, val_acc: 0.8469\n",
            "Epoch [3], val_loss: 0.6853, val_acc: 0.8484\n",
            "Epoch [4], val_loss: 0.6722, val_acc: 0.8502\n",
            "Epoch [5], val_loss: 0.6602, val_acc: 0.8511\n",
            "Epoch [6], val_loss: 0.6491, val_acc: 0.8530\n",
            "Epoch [7], val_loss: 0.6388, val_acc: 0.8539\n",
            "Epoch [8], val_loss: 0.6293, val_acc: 0.8550\n",
            "Epoch [9], val_loss: 0.6203, val_acc: 0.8560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5ALmwZjfTTa",
        "outputId": "73d1c652-5d8b-4288-fa4b-f1e48dd829d2"
      },
      "source": [
        "history4 = fit(15, 0.0001, model, train_loader, val_loader)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], val_loss: 0.6195, val_acc: 0.8563\n",
            "Epoch [1], val_loss: 0.6186, val_acc: 0.8565\n",
            "Epoch [2], val_loss: 0.6178, val_acc: 0.8566\n",
            "Epoch [3], val_loss: 0.6169, val_acc: 0.8566\n",
            "Epoch [4], val_loss: 0.6161, val_acc: 0.8568\n",
            "Epoch [5], val_loss: 0.6153, val_acc: 0.8568\n",
            "Epoch [6], val_loss: 0.6144, val_acc: 0.8569\n",
            "Epoch [7], val_loss: 0.6136, val_acc: 0.8572\n",
            "Epoch [8], val_loss: 0.6128, val_acc: 0.8572\n",
            "Epoch [9], val_loss: 0.6120, val_acc: 0.8572\n",
            "Epoch [10], val_loss: 0.6112, val_acc: 0.8573\n",
            "Epoch [11], val_loss: 0.6104, val_acc: 0.8575\n",
            "Epoch [12], val_loss: 0.6096, val_acc: 0.8575\n",
            "Epoch [13], val_loss: 0.6088, val_acc: 0.8575\n",
            "Epoch [14], val_loss: 0.6080, val_acc: 0.8576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCm1903IfTTa"
      },
      "source": [
        "Aunque la precisiÃ³n va mejorando cuando continuamos el entrenamiento, las mejoras son cada vez mÃ¡s pequeÃ±as. \r\n",
        "\r\n",
        "Veamos esto en un grÃ¡fico:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "swNRQSa0fTTa",
        "outputId": "2ebf040e-db4a-45f9-9415-df8d5f720732"
      },
      "source": [
        "history = [result0] + history1 + history2 + history3 + history4\n",
        "accuracies = [result['val_acc'] for result in history]\n",
        "plt.plot(accuracies, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('Accuracy vs. No. of epochs');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-a7zRGxfTTa"
      },
      "source": [
        "A la vista de los resultados obtenidos y del grÃ¡fico, se aprecia fÃ¡cilmente que probablemente nuestro modelo no pueda ir mucho mÃ¡s allÃ¡ dde un 905 de precisiÃ³n, incluso despuÃ©s de entrenar bastantes ciclos. Una posible explicaciÃ³n para esto es que tengamos una tasa de aprendizaje (_learning rate_) alta y sea muy dificil acceder a esas pequeÃ±as mejoras debido al tamaÃ±o del  \"paso\" (recordemos el grÃ¡fico de la curva del error y el gradiente). Los valores obtenidos pueden estar dando vueltas alrededor del punto Ã³ptimo con el mÃ­nimo error. Se debe probar diferentes tasas de aprendizaje durante mÃ¡s ciclos de entrenamiento para observar si eso mejora. \n",
        "\n",
        "Sin embargo, otra razÃ³n, probablemente mÃ¡s importante es que **el modelo no es lo bastante potente**. Si recordamos la hipÃ³tesis inicial planteada al plantear el modelo, hemos asumido que la salida (en este caso las probabilidades de cada clase de salida) es una **funciÃ³n lineal** de las entradas (intensidades de los pÃ­xels), ponderadas por unos pesos y on un factor de ajuste al que llamamos bias. Pero eso es una presunciÃ³n muy dÃ©bil, pues no tiene porquÃ© existir una relacion lineal entre la intensidad de los pÃ­xels en una imagen y el nÃºmero que representan. Es por ello que aunque funciona razonablemente bien con un pequeÃ±o conjunto como MNIST (se ha obtenido en torno al 85%), necesitaremos modelos mÃ¡s sofÃ­sticados que sean capaces de captar tambiÃ©n relaciones no lineales entre las variables de entrada y sus etiquetas, de forma que puedan ser aplicados en tareas mÃ¡s complejas como clasificaciÃ³n avanzada, reconocimiento de objetos, personas, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXgOorOIfTTa"
      },
      "source": [
        "## Probando con imÃ¡genes individuales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qUQKIJefTTa"
      },
      "source": [
        "Una vez que hemos explorado los resultados y la precisiÃ³n del modelo entrenado, tambiÃ©n es necesario ver que resultados produce sobre imÃ¡genes nuevas. \r\n",
        "Vamos a probar el modelo con algunas del conjunto de test de 10000 imÃ¡genes que como sabemos, no ha intervenido en ningun momento en el entrenamiento. Empezaremos por generar el test dataset empleando el metodo transform de `ToTensor`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqcHUQK3fTTa"
      },
      "source": [
        "# Define test dataset\n",
        "test_dataset = MNIST(root='data/', \n",
        "                     train=False,\n",
        "                     transform=transforms.ToTensor())"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw9J_viefTTa"
      },
      "source": [
        "Revisamos algun eejemplo del conjunto test. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "yzEdFHq6fTTa",
        "outputId": "4a42c8e7-41d0-4e26-e197-4f232c468260"
      },
      "source": [
        "img, label = test_dataset[10]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Shape:', img.shape)\n",
        "print('Label:', label)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: torch.Size([1, 28, 28])\n",
            "Label: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgwbsDUjfTTa"
      },
      "source": [
        "Podemos ahora definir una funciÃ³n auxiliar `predict_image`, que devolverÃ¡ la etiqueta obtenida para una sola imagen, proporcionada en un tensor individual. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FfeYQaOfTTa"
      },
      "source": [
        "def predict_image(img, model):\n",
        "    xb = img.unsqueeze(0)\n",
        "    yb = model(xb)\n",
        "    _, preds = torch.max(yb, dim=1)\n",
        "    return preds[0].item()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ppgosBYfTTa"
      },
      "source": [
        "`img.unsqueeze` lo que hace es aÃ±adir una dimensiÃ³n adicional al principio del tensor de la imagen 1x28x28, convirtiendolo en un tensor de 1x1x28x28, que el modelo interpretarla como un lote conteniendo una sola imagen\n",
        "\n",
        "Probamos con varias imÃ¡genes: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "sPNcp52ifTTa",
        "outputId": "2cdae469-639b-4235-8a51-8793133faf9e"
      },
      "source": [
        "img, label = test_dataset[0]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 7 , Predicted: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAM3ElEQVR4nO3dXahc9bnH8d/vpCmI6UXiS9ik0bTBC8tBEo1BSCxbQktOvIjFIM1FyYHi7kWUFkuo2It4WaQv1JvALkrTkmMJpGoQscmJxVDU4o5Es2NIjCGaxLxYIjQRJMY+vdjLso0za8ZZa2ZN8nw/sJmZ9cya9bDMz7VmvczfESEAV77/aroBAINB2IEkCDuQBGEHkiDsQBJfGeTCbHPoH+iziHCr6ZW27LZX2j5o+7Dth6t8FoD+cq/n2W3PkHRI0nckHZf0mqS1EfFWyTxs2YE+68eWfamkwxFxJCIuSPqTpNUVPg9AH1UJ+zxJx6a9Pl5M+xzbY7YnbE9UWBaAivp+gC4ixiWNS+zGA02qsmU/IWn+tNdfL6YBGEJVwv6apJtsf8P2VyV9X9L2etoCULeed+Mj4qLtByT9RdIMSU9GxP7aOgNQq55PvfW0ML6zA33Xl4tqAFw+CDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ9Dw+uyTZPirpnKRPJV2MiCV1NAWgfpXCXrgrIv5Rw+cA6CN244EkqoY9JO2wvcf2WKs32B6zPWF7ouKyAFTgiOh9ZnteRJywfb2knZIejIjdJe/vfWEAuhIRbjW90pY9Ik4Uj2ckPS1paZXPA9A/PYfd9tW2v/bZc0nflTRZV2MA6lXlaPxcSU/b/uxz/i8iXqilKwC1q/Sd/UsvjO/sQN/15Ts7gMsHYQeSIOxAEoQdSIKwA0nUcSNMCmvWrGlbu//++0vnff/990vrH3/8cWl9y5YtpfVTp061rR0+fLh0XuTBlh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuCuty4dOXKkbW3BggWDa6SFc+fOta3t379/gJ0Ml+PHj7etPfbYY6XzTkxcvr+ixl1vQHKEHUiCsANJEHYgCcIOJEHYgSQIO5AE97N3qeye9VtuuaV03gMHDpTWb7755tL6rbfeWlofHR1tW7vjjjtK5z127Fhpff78+aX1Ki5evFha/+CDD0rrIyMjPS/7vffeK61fzufZ22HLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcD/7FWD27Nlta4sWLSqdd8+ePaX122+/vaeeutHp9/IPHTpUWu90/cKcOXPa1tavX18676ZNm0rrw6zn+9ltP2n7jO3JadPm2N5p++3isf2/NgBDoZvd+N9LWnnJtIcl7YqImyTtKl4DGGIdwx4RuyWdvWTyakmbi+ebJd1Tc18AatbrtfFzI+Jk8fyUpLnt3mh7TNJYj8sBUJPKN8JERJQdeIuIcUnjEgfogCb1eurttO0RSSoez9TXEoB+6DXs2yWtK56vk/RsPe0A6JeO59ltPyVpVNK1kk5L2ijpGUlbJd0g6V1J90XEpQfxWn0Wu/Ho2r333lta37p1a2l9cnKybe2uu+4qnffs2Y7/nIdWu/PsHb+zR8TaNqUVlToCMFBcLgskQdiBJAg7kARhB5Ig7EAS3OKKxlx//fWl9X379lWaf82aNW1r27ZtK533csaQzUByhB1IgrADSRB2IAnCDiRB2IEkCDuQBEM2ozGdfs75uuuuK61/+OGHpfWDBw9+6Z6uZGzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJ7mdHXy1btqxt7cUXXyydd+bMmaX10dHR0vru3btL61cq7mcHkiPsQBKEHUiCsANJEHYgCcIOJEHYgSS4nx19tWrVqra1TufRd+3aVVp/5ZVXeuopq45bdttP2j5je3LatEdtn7C9t/hr/18UwFDoZjf+95JWtpj+m4hYVPw9X29bAOrWMewRsVvS2QH0AqCPqhyge8D2m8Vu/ux2b7I9ZnvC9kSFZQGoqNewb5K0UNIiSScl/ardGyNiPCKWRMSSHpcFoAY9hT0iTkfEpxHxL0m/k7S03rYA1K2nsNsemfbye5Im270XwHDoeJ7d9lOSRiVda/u4pI2SRm0vkhSSjkr6UR97xBC76qqrSusrV7Y6kTPlwoULpfNu3LixtP7JJ5+U1vF5HcMeEWtbTH6iD70A6CMulwWSIOxAEoQdSIKwA0kQdiAJbnFFJRs2bCitL168uG3thRdeKJ335Zdf7qkntMaWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYMhmlLr77rtL688880xp/aOPPmpbK7v9VZJeffXV0jpaY8hmIDnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC+9mTu+aaa0rrjz/+eGl9xowZpfXnn28/5ifn0QeLLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMH97Fe4TufBO53rvu2220rr77zzTmm97J71TvOiNz3fz257vu2/2n7L9n7bPy6mz7G90/bbxePsupsGUJ9uduMvSvppRHxL0h2S1tv+lqSHJe2KiJsk7SpeAxhSHcMeEScj4vXi+TlJByTNk7Ra0ubibZsl3dOvJgFU96Wujbe9QNJiSX+XNDciThalU5LmtplnTNJY7y0CqEPXR+Ntz5K0TdJPIuKf02sxdZSv5cG3iBiPiCURsaRSpwAq6SrstmdqKuhbIuLPxeTTtkeK+oikM/1pEUAdOu7G27akJyQdiIhfTyttl7RO0i+Kx2f70iEqWbhwYWm906m1Th566KHSOqfXhkc339mXSfqBpH229xbTHtFUyLfa/qGkdyXd158WAdShY9gj4m+SWp6kl7Si3nYA9AuXywJJEHYgCcIOJEHYgSQIO5AEPyV9Bbjxxhvb1nbs2FHpszds2FBaf+655yp9PgaHLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMF59ivA2Fj7X/264YYbKn32Sy+9VFof5E+Roxq27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOfZLwPLly8vrT/44IMD6gSXM7bsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEN+Ozz5f0B0lzJYWk8Yj4re1HJd0v6YPirY9ExPP9ajSzO++8s7Q+a9asnj+70/jp58+f7/mzMVy6uajmoqSfRsTrtr8maY/tnUXtNxHxy/61B6Au3YzPflLSyeL5OdsHJM3rd2MA6vWlvrPbXiBpsaS/F5MesP2m7Sdtz24zz5jtCdsTlToFUEnXYbc9S9I2ST+JiH9K2iRpoaRFmtry/6rVfBExHhFLImJJDf0C6FFXYbc9U1NB3xIRf5akiDgdEZ9GxL8k/U7S0v61CaCqjmG3bUlPSDoQEb+eNn1k2tu+J2my/vYA1KWbo/HLJP1A0j7be4tpj0haa3uRpk7HHZX0o750iEreeOON0vqKFStK62fPnq2zHTSom6Pxf5PkFiXOqQOXEa6gA5Ig7EAShB1IgrADSRB2IAnCDiThQQ65a5vxfYE+i4hWp8rZsgNZEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoMesvkfkt6d9vraYtowGtbehrUvid56VWdvN7YrDPSimi8s3J4Y1t+mG9behrUvid56Naje2I0HkiDsQBJNh3284eWXGdbehrUvid56NZDeGv3ODmBwmt6yAxgQwg4k0UjYba+0fdD2YdsPN9FDO7aP2t5ne2/T49MVY+idsT05bdoc2zttv108thxjr6HeHrV9olh3e22vaqi3+bb/avst2/tt/7iY3ui6K+lrIOtt4N/Zbc+QdEjSdyQdl/SapLUR8dZAG2nD9lFJSyKi8QswbH9b0nlJf4iI/y6mPSbpbET8ovgf5eyI+NmQ9PaopPNND+NdjFY0Mn2YcUn3SPpfNbjuSvq6TwNYb01s2ZdKOhwRRyLigqQ/SVrdQB9DLyJ2S7p0SJbVkjYXzzdr6h/LwLXpbShExMmIeL14fk7SZ8OMN7ruSvoaiCbCPk/SsWmvj2u4xnsPSTts77E91nQzLcyNiJPF81OS5jbZTAsdh/EepEuGGR+addfL8OdVcYDui5ZHxK2S/kfS+mJ3dSjF1HewYTp32tUw3oPSYpjx/2hy3fU6/HlVTYT9hKT5015/vZg2FCLiRPF4RtLTGr6hqE9/NoJu8Xim4X7+Y5iG8W41zLiGYN01Ofx5E2F/TdJNtr9h+6uSvi9pewN9fIHtq4sDJ7J9taTvaviGot4uaV3xfJ2kZxvs5XOGZRjvdsOMq+F11/jw5xEx8D9JqzR1RP4dST9vooc2fX1T0hvF3/6me5P0lKZ26z7R1LGNH0q6RtIuSW9L+n9Jc4aotz9K2ifpTU0Fa6Sh3pZrahf9TUl7i79VTa+7kr4Gst64XBZIggN0QBKEHUiCsANJEHYgCcIOJEHYgSQIO5DEvwEvYRv57rmVLgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Nrj-w1COfTTa",
        "outputId": "1eb57a47-dc36-419a-a75f-3c770cc61aed"
      },
      "source": [
        "img, label = test_dataset[10]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 0 , Predicted: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "EPrF7gB4fTTb",
        "outputId": "10b0327d-1dd9-4648-aa8d-a1df912b807f"
      },
      "source": [
        "img, label = test_dataset[193]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 9 , Predicted: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "eetYJdoFfTTb",
        "outputId": "52c3c2c8-40d4-4a09-e856-ab44595f34d7"
      },
      "source": [
        "img, label = test_dataset[1839]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 2 , Predicted: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ6Wsk4TfTTb"
      },
      "source": [
        "Identificando donde lo hace peor nuestro modelo nos puede orientar a mejorar el modelo bien recogiendo mÃ¡s datos de entrenamiento, ajaustando los hiperparÃ¡metros, incrementando/ o reduciendo la complejidad del modelo, etc.\n",
        "\n",
        "Como paso final, vamos a obtener los valores de loss y accuracy del modelo sobre todo el conjunto de test. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bdjaeG-fTTb",
        "outputId": "10cb383c-67aa-42a5-f944-72f5f07da032"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=256)\n",
        "result = evaluate(model, test_loader)\n",
        "result"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.8681640625, 'val_loss': 0.5735012292861938}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfe2IzF3fTTb"
      },
      "source": [
        "Lso resultados obtenidos con el conjunto de test nos pueden orientar sobre las  formas de afrontar una mejora. Si los resultados son muy distintos, es posible que el conjunto de test no estÃ© correctamente muestreado y por tanto no tenga similar distribuciÃ³n a los empleados en el entrenamiento. TambiÃ©n se pueden detectar situaciones de sobreajuste en el entrenamiento. En general, valores similares en entrenamiento y test indican que al menos metodolÃ³gicamente el modelo estÃ¡ bien planteado, si bien puede necesitar mejoras en parÃ¡metros, arquitectura, etc. para aumentar sus resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqC3HEZJfTTb"
      },
      "source": [
        "## Salvar y cargar el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyE7vx_zfTTb"
      },
      "source": [
        "Una vez entrenado el modelo (lo cual puede haber requerido mucho tiempo) y obtenido unos resultados aceptables, es importante poder guardar los pesos y biases obtenidos, de forma que podamos recuperarlos y reutilizarlos cuando queramos, evitando asÃ­ tener que reentrenar desde cero. \r\n",
        "\r\n",
        "Una forma de grabar el modelo serÃ¡ la siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xESC-106fTTb"
      },
      "source": [
        "torch.save(model.state_dict(), 'mnist-logistic.pth')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAHSyFLPfTTb"
      },
      "source": [
        "El mÃ©todo `.state_dict` retorna un `OrderedDict` conteniendo las matrices con los pesos y biases mapeados del modelo que hemos especificado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elbp48SCfTTb",
        "outputId": "49a877f9-2714-49e2-f7b9-279d30b0db6e"
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[-0.0336, -0.0045,  0.0063,  ..., -0.0064,  0.0209, -0.0070],\n",
              "                      [-0.0059, -0.0071,  0.0048,  ..., -0.0297, -0.0181, -0.0351],\n",
              "                      [-0.0041, -0.0023,  0.0307,  ..., -0.0100, -0.0277,  0.0215],\n",
              "                      ...,\n",
              "                      [-0.0054, -0.0341,  0.0073,  ...,  0.0156,  0.0229, -0.0054],\n",
              "                      [-0.0294, -0.0131, -0.0054,  ...,  0.0030, -0.0170,  0.0008],\n",
              "                      [-0.0244,  0.0300,  0.0175,  ...,  0.0094, -0.0264,  0.0123]])),\n",
              "             ('linear.bias',\n",
              "              tensor([-0.0595,  0.1336, -0.0386, -0.0632,  0.0444,  0.0861,  0.0141,  0.0418,\n",
              "                      -0.0832, -0.0042]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PKRLwDPV3nK"
      },
      "source": [
        "**NOTA IMPORTANTE**: si ejecutas el mÃ©todo `torch.sav`en Colab, el fichero generado se graba temporalmente en el directorio de datos del cuaderno. SerÃ¡ necesario descargarlo a un equipo local o trasladarlo a otra ubicaciÃ³n permanente o se perderÃ¡ cuando finalicemos la sesiÃ³n. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc9kRTDpfTTb"
      },
      "source": [
        "Para cargar los parÃ¡metros del modelo, podemos instanciar un nuevo objeto de la clase inicial `MnistModel`, y a continuaciÃ³n emplear el mÃ©todo `.load_state_dict`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO666r7_1rbW"
      },
      "source": [
        "model2 = MnistModel()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUvYwe4Q1six",
        "outputId": "d75b5187-ef76-43b5-92d1-92fd91ba7718"
      },
      "source": [
        "# inicalmente los parÃ¡metros son los que se han adjudicado aleatoriamente al instanciar el objeto\r\n",
        "model2.state_dict()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[-0.0175, -0.0159, -0.0026,  ..., -0.0044,  0.0047,  0.0137],\n",
              "                      [ 0.0074, -0.0178,  0.0313,  ...,  0.0169, -0.0295,  0.0166],\n",
              "                      [ 0.0315,  0.0341, -0.0114,  ...,  0.0076,  0.0091, -0.0028],\n",
              "                      ...,\n",
              "                      [ 0.0026, -0.0143,  0.0301,  ...,  0.0340,  0.0273, -0.0256],\n",
              "                      [-0.0123, -0.0114,  0.0341,  ..., -0.0149, -0.0232, -0.0185],\n",
              "                      [ 0.0025,  0.0342, -0.0125,  ...,  0.0325,  0.0083,  0.0259]])),\n",
              "             ('linear.bias',\n",
              "              tensor([-0.0171, -0.0126,  0.0228, -0.0236,  0.0322, -0.0208, -0.0101, -0.0127,\n",
              "                       0.0031,  0.0279]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHac9s6e1vyS",
        "outputId": "5cd9e38a-c6e9-402d-fff9-c40973e40875"
      },
      "source": [
        "evaluate(model2, test_loader)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.13437500596046448, 'val_loss': 2.3092026710510254}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvR1g8ggfTTb",
        "outputId": "a22cd455-fe21-4cc0-88ed-098cfcd8e574"
      },
      "source": [
        "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
        "model2.state_dict()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[-0.0336, -0.0045,  0.0063,  ..., -0.0064,  0.0209, -0.0070],\n",
              "                      [-0.0059, -0.0071,  0.0048,  ..., -0.0297, -0.0181, -0.0351],\n",
              "                      [-0.0041, -0.0023,  0.0307,  ..., -0.0100, -0.0277,  0.0215],\n",
              "                      ...,\n",
              "                      [-0.0054, -0.0341,  0.0073,  ...,  0.0156,  0.0229, -0.0054],\n",
              "                      [-0.0294, -0.0131, -0.0054,  ...,  0.0030, -0.0170,  0.0008],\n",
              "                      [-0.0244,  0.0300,  0.0175,  ...,  0.0094, -0.0264,  0.0123]])),\n",
              "             ('linear.bias',\n",
              "              tensor([-0.0595,  0.1336, -0.0386, -0.0632,  0.0444,  0.0861,  0.0141,  0.0418,\n",
              "                      -0.0832, -0.0042]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhjBm4BMfTTb"
      },
      "source": [
        "Podemos comprobar que los parÃ¡metros que tenemos son los que habiamos grabado, y comprobamos que obtenemos en los mismos resultados de loss y accuracy en el conjunto de test, igual que antes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UynZ4aSLfTTb",
        "outputId": "e57a560f-c8ff-418d-a485-61ade1b36977"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=256)\n",
        "result = evaluate(model2, test_loader)\n",
        "result"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.8681640625, 'val_loss': 0.5735012292861938}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnz8WyThaPOP"
      },
      "source": [
        "Observamos que los valores obtenidos son exactamente los mismos que obtuvimos al realizar la evaluaciÃ³n con el modelo `model` varias celdas mÃ¡s arriba, lo cual indica que los pesos se han cargado correctamente y que nuestro `modelo2` lo estÃ¡ haciendo tan bien como el modelo original de donde proceden los valores de los parÃ¡metros. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2QOf-HEltUP"
      },
      "source": [
        "## Fin del Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZDr1zYmBkRV"
      },
      "source": [
        "Referencias y modelos empleados para el Notebook: \r\n",
        "\r\n",
        "*   DocumentaciÃ³n de [Pytorch](https://pytorch.org/docs/stable/index.html) \r\n",
        "*   [PyTorch Tutorial for Deep Learning Researchers](https://github.com/yunjey/pytorch-tutorial) by Yunjey Choi\r\n",
        "*   [FastAI](https://www.fast.ai/) development notebooks by Jeremy Howard.\r\n",
        "*   DocumentaciÃ³n y cursos en [Pierian Data](https://www.pieriandata.com/)\r\n",
        "*   Tutoriales y notebooks del curso \"Deep Learning with Pytorch: Zero to GANs\" de [Aakash N S](https://jovian.ai/aakashns)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    }
  ]
}